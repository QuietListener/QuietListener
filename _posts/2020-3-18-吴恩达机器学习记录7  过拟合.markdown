---
layout: post
title: 吴恩达机器学习记录7 过拟合
date: 2020-3-23 14:32:00
categories:  机器学习
---



<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

## 1. 欠拟合，过拟合
1. 欠拟合(underfitting): 对训练数据，欠拟合会有很大的偏差(bias)
2. 过拟合(overfitting): 对训练数据，过拟合会有很小的偏差，但是对新数据预测能力不好

下面就是一个例子，第1个是欠拟合，第3个是过拟合:

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200324-regularation1.jpg" width="500"> 


逻辑回归也是一样的   

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200324-regularation2.jpg" width="500"> 



## 2. 什么时候会产生(过拟合)overfitting
### 1. 有很多特性(feature),但是我们的训练数据很少。
比如，放假预测时候，feature有面积，楼龄，是否靠近卢边，有没有贷款等很多feature.但是我们的训练样本又很少，就容易产生过拟合。
### 2. 解决办法
#### 1. 第一类
1. 舍弃一部分feature    
**缺点**:减少过拟合的概率的同事，**也舍弃一些有用的信息**
2. 使用模型选择算法


#### 2.第二类:
1. 使用正则化(regularization)   

这种方法保留所有的feature，如果某个feature对结果预测关系不大， 训练过程会**减小某个参数的系数$$θ_j$$**。这样减小这个feature对预测的影响。



## 3. regularization 背后的原理
  模型中feature的参数 为
  $$θ_1,θ_2,θ_3,...,θ_n $$ 

  1. **这些参数越小，表明模型越简单。**
  2. **越简单的模型，更不容易overfitting**


## 4. 推导

### 1. **原来的问题是:**
   $$J(θ) = \frac1{2m} \sum_{i=1}^{m} [  h_θ(x^{(i)} - y^{(i) }) ^2 ] $$    
  并且 最小化 J(θ) 获取 θ值


### 2. **现在的问题是:**
   $$J_1(θ) = \frac1{2m} \sum_{i=1}^{m} [  h_θ(x^{(i)}） - y^{(i) }) ^2   + λ \sum_{i=1}^m θ_i^2] $$    

 最小化 $$J_1(θ)$$ 获取 θ值    

  $$ λ \sum_{i=1}^m θ_i^2$$  叫做**正则化项**。
  
  **λ叫做正则化参数。**  λ其实就是控制 $$ h_θ(x^{(i)} - y^{(i) }) ^2 $$ 和  $$  \sum_{i=1}^m θ_i^2$$ 之间的取舍，在二者之间做一个trade off。
  例如
  我们的$$h_θ(x) = θ_0 + θ_1x+θ_2x^2 + θ_1x+θ_3x^3 + θ_4x+θ_4x^4 $$

  如果我们将 λ 设置得非常大，比如10000000，最后训练出来的所有参数 $$θ_1,θ_2,θ_3,...,θ_n $$ 都接近0，那曲线就变成了 $$h_θ(x) = θ_0$$ **成了一条直线** ,又成了**欠拟合(underfitting)**如下图：


<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200324-regularation3.jpg" width="400"> 


  **所以选取适合的 λ 也是非常重要的问题**




## 5. 线性回归的正则化
### 1. 梯度下降的正则化 
1. 原来的迭代如下:

$$ θ_j := θ_j - α \frac{1}{m} \sum_{i=1}^m (h_θ(x^{(i)}) - y^{{i}})x_j^{(i)} $$

2. 添加正则化后:    

$$ θ_j := θ_j - α [\frac{1}{m} \sum_{i=1}^m (h_θ(x^{(i)}) - y^{{i}})x_j^{(i)} + \fracλm θ_j] $$

3. 化简一下:   

$$ θ_j := (1-α\fracλm)θ_j - α \frac{1}{m} \sum_{i=1}^m (h_θ(x^{(i)}) - y^{{i}})x_j^{(i)} $$

我们可以看到  与没有正则化相比: 只有 $$(1-α\fracλm)θ_j$$不一样，$$ 0<1-α\fracλm<1$$ ,所以其实加了正则化后，**每一次迭代都在减小$$θ_j $$， 以至于不让它变得太大，这样降低了模型复杂度**。

### 2. 正规化的正则化
正规化可以用下面公式一步算出θ的值:  
$$ θ = (X^TX)^{-1} $$
但是当feature比训练数据多时候，$$X^TX$$可能不可逆。  
加入正则化后可以保证$$X^TX$$**是可逆的**。
如下图

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200324-regularation4.jpg" width="400"> 




## 6. 逻辑回归的正则化
跟线性回归一样。

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200324-regularation5.jpg" width="400"> 



<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200324-regularation6.jpg" width="400"> 
