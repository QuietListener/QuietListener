---
layout: post
title: redis运维 备忘
date: 2020-9-9 14:32:00
categories:  java maven
---
### 1. 持久化 RDB AOF
#### 1. redis持久化的机制 
redis有RDB和AOF两种持久化机制，下次重启时候利用之前持久化的文件即可恢复数据。

#### 2.RDB
RDB是把当前**进程数据生成快照保存到硬盘**的过程，可以手动也可以自动触发。

##### 1.手动rdb
```shell
127.0.0.1:6379> set aabb ccdd
OK
127.0.0.1:6379> bgsave
Background saving started

```

log 信息
```java
8906:M 10 Sep 10:03:38.417 * Background saving started by pid 8995
8995:C 10 Sep 10:03:38.568 * DB saved on disk
8995:C 10 Sep 10:03:38.568 * RDB: 6 MB of memory used by copy-on-write
8906:M 10 Sep 10:03:38.643 * Background saving terminated with success

```


##### 2.自动rdb
```shell
#rdb 10秒钟有3次修改就作rdb
save 10 3
```

执行三次set   

```shell
127.0.0.1:6379> 
127.0.0.1:6379> set aabb ccdd
OK
127.0.0.1:6379> set aabb ccdd1
OK
127.0.0.1:6379> set aabb ccdd12
OK
127.0.0.1:6379> set aabb ccdd124
OK
```

log信息   

```java
9184:M 10 Sep 10:13:03.227 * Background saving started by pid 9195
9195:C 10 Sep 10:13:03.240 * DB saved on disk
9195:C 10 Sep 10:13:03.242 * RDB: 0 MB of memory used by copy-on-write
9184:M 10 Sep 10:13:03.328 * Background saving terminated with success

```

##### 3. rdb过程
rdb是使用fork一个子进程在后台执行的。
1. 判断是否已经有rdb/aof子进程，如果存在就返回。
2. fork一个子进程，fork会阻塞父进程
3. 父进程fork完成后，bgsave返回Background saving stated.
4. 子进程根据父进程内存生成临时快照文件，并对原文件做快照替换。
5. 进程发送信号给父进程表示完成。


下面是状态。
```shell
127.0.0.1:6379> info stats
# Stats
total_connections_received:6
total_commands_processed:100007
instantaneous_ops_per_sec:0
total_net_input_bytes:10089094
total_net_output_bytes:504551
instantaneous_input_kbps:0.00
instantaneous_output_kbps:0.00
rejected_connections:0
sync_full:0
sync_partial_ok:0
sync_partial_err:0
expired_keys:0
evicted_keys:51083
keyspace_hits:0
keyspace_misses:0
pubsub_channels:0
pubsub_patterns:0
latest_fork_usec:322  #上一次fork 使用了 322微妙(阻塞了父进程322微秒)
migrate_cached_sockets:0
127.0.0.1:6379> 

```


```shell
127.0.0.1:6379> info Persistence
# Persistence
loading:0
rdb_changes_since_last_save:0
rdb_bgsave_in_progress:0
rdb_last_save_time:1599708110 #上次开始rdb的时间
rdb_last_bgsave_status:ok
rdb_last_bgsave_time_sec:0
rdb_current_bgsave_time_sec:-1
aof_enabled:0
aof_rewrite_in_progress:0
aof_rewrite_scheduled:0
aof_last_rewrite_time_sec:-1
aof_current_rewrite_time_sec:-1
aof_last_bgrewrite_status:ok
aof_last_write_status:ok
127.0.0.1:6379> 
```

##### 4 rdb优缺点
###### 优点
1. rdb是**压缩**过的二进制文件，是一个时间点的快照。适用于全量备份，全量复制等场景，用于灾难恢复。
2. redis加载rdb也远远快于AOF

###### 缺点
1. 无法实时的持久化，或者秒级持久化。
bgsave需要fork进程，非常耗时，线上8G内存的redis实例，fork需要200毫秒。如果一个qps是5万的话，就影响了1/5，大约1万个请求。


#### 3.AOF(append of file)
##### 1. 原理
aof以独立日志的方式记录每次写命令，重启时候在重新执行aof文件中的命令达到恢复目的。
**aof解决了吃鸡化的实时性**

#### 2#. aof工作流程
1. 写入命令写到aof_buf
2. aof缓冲根据**策略**向硬盘做**同步**操作
3. aof文件越来越大，需要定期对aof重写，达到压缩的目的
4. redis重启时候，可以加在aof文件进行数据恢复

##### 注意事项
1. aof使用文本协议格式
有很好的兼容性和可读性。
2. aof不直接写入磁盘，使用aof_buf 缓冲区
单线程，防止io阻塞，可以有不同的策略将缓冲同步到磁盘，在性能和安全性做出平衡。

##### 3. aof缓冲区同步策略
1. alaways 命令写aof_buf后调用fsync同步到磁盘文件，然后再返回。
2. everysec 命令写入aof_buf后调用write操作，立即返回，由一个单独的同步线程每秒做一次fsync同步。
3. no 命令写入aof_buf后调用write操作，立即返回，不对aof文件做fsync同步。

**建议使用everysec**，如果突然宕机，值丢失1秒左右的数据。

##### 4. aof文件重写
命令不断写入 aof会越来越大，重写机制可以解决这个问题。 
aof文件变小了 redis加载aof文件也会更快。

###### 1. 为什么重写可以使得命令变小
1. 进程内超时的数据可以不用写了。
2. 多条命令可以合并，比如 lpush l a , lpush l b 可以合并为 lpush l a b
3. 旧的aof文件里的无效命令已经可以不用写了。

#### aof重写触发时机
##### 1. 手动触发
bgrewriteaof命令

##### 2. 自动触发
```shell
#aof
appendonly yes
appendfilename "aof6379.aof"
#重写
#同时满足下面2个才进行重写
#当aof文件大于64M才重写
auto-aof-rewrite-min-size 40M
#当aof比上一次aof后的文件大的比例，比如上次是100m percentage设置为20  120M的时候就可以进行重写了。
auto-aof-rewrite-percentage  20

```


下面是自动开始aof rewrite的过程
```shell
13195:M 10 Sep 14:41:23.925 * Starting automatic rewriting of AOF on 6405058400% growth
13195:M 10 Sep 14:41:23.926 * Background append only file rewriting started by pid 13404
13195:M 10 Sep 14:41:25.071 * AOF rewrite child asks to stop sending diffs.
13404:C 10 Sep 14:41:25.071 * Parent agreed to stop sending diffs. Finalizing AOF...
13404:C 10 Sep 14:41:25.071 * Concatenating 0.48 MB of AOF diff received from parent.
13404:C 10 Sep 14:41:25.098 * SYNC append only file rewrite performed
13404:C 10 Sep 14:41:25.099 * AOF rewrite: 9 MB of memory used by copy-on-write #在fork后 又有9M的内存修改了，所以有9M的内存做了复制
13195:M 10 Sep 14:41:25.214 * Background AOF rewrite terminated with success
13195:M 10 Sep 14:41:25.214 * Residual parent diff successfully flushed to the rewritten AOF (0.62 MB)
13195:M 10 Sep 14:41:25.215 * Background AOF rewrite finished successfully

```

重写aof文件后问价大小的对比,**aof文件从60多M变为了13M**
```shell
[www@localhost tests]$ ls data -lah
总用量 71M
drwxrwxr-x. 2 www www   43 9月  10 14:41 .
drwxrwxr-x. 4 www www   65 9月  10 14:40 ..
-rw-r--r--. 1 www www  61M 9月  10 14:41 aof6379.aof
-rw-rw-r--. 1 www www 5.5M 9月  10 14:41 dump6379.rdb
[www@localhost tests]$ ls data -lah
总用量 19M
drwxrwxr-x. 2 www www   43 9月  10 14:41 .
drwxrwxr-x. 4 www www   65 9月  10 14:40 ..
-rw-rw-r--. 1 www www  13M 9月  10 14:41 aof6379.aof
-rw-rw-r--. 1 www www 2.8M 9月  10 14:41 dump6379.rdb
```

##### 3. aof rewrite的过程
1. 如果正在执行aof 直接返回
2. 如果正在执行bgsave要等到bgsave完成后再执行 aof重写
3. 父进程fock子进程，与bgsave开销一样。
4. 1 主进程继续响应命令，将写命令写入 aof缓冲区，并根据策略做同步处理
4. 2 子进程只能看到fork时候的内存快照，fock后父进程响应的写命令修改的这些新数据，子进程看不见。 redis使用 aof重写缓冲区来保存这部分新数据。防止数据丢失。
5. 子进程根据内存快照，按照命令合并等规则写入新的aof文件。每次写入aof-rewrite-incremental-fsync 字节(默认32M)的数据。防止磁盘阻塞
6. 1 先的aof文件写入后，发送信号给父进程。
6. 2 父进程把aof重写缓冲区的数据写入到新的aof文件。
6. 3 替换老的aof文件。


### 重启后持久化文件加载
 优先加载aof
 ![部署](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/redis-aof-rdb-reload.png) 


### 优化的地方
#### 1. fork
##### 问题
1. aof或者rdb都需要操作系统fork操作。fork是很重的一个操作。
2. fork时候不需要复制父进程的内存空间，但要复制父进程的**内存页**，10G内存也需要复制20Md内存页。
3. 经验值是10G内存大概需要20毫秒fork时间。


##### 怎么优化
1. 单个redis实例最好不要操作10G内存。
2. redis尽量使用物理机，使用用fork很高效的虚拟机。不用虚拟机。
3. 降低fork频率，不使用自动rdb，aof 合理配置 减少不必要的全量复制。


### rdb或者aof重写时的 cpu 内存 磁盘
#### 1.cpu和磁盘
子进程在重写或者rdb时候，将进程内的数据粉笔写入文件，这个过程视乎cpu密集型，也是io密集型。
**优化**
1. 一台机器上如果有多个redis实例，最好保证同一时刻只有一个实例在做rdb或者aof重写操作。
2. 每一个redis实例，将aof/rdb文件放在不同的磁盘。
3. no-appendfsync-on-rewrite yes 这样也有了丢失整个aof重写期间的数据的风险。


#### 2. 内存
linux有copy-on-write机制，当父进程处理写请求时候会将要修改的内存页创建副本。这是一部分开销。

一篇文章
https://cloud.tencent.com/developer/article/1633077

#### 3 aof追加阻塞
主要是磁盘io问题

### 部署
1. 不要在同一台机器上部署io密集型的服务。例如 kafka
2. aof重写不要自动执行，可以外部监控，然后在适当的时候手动做aof重写，比如凌晨或者业务低风时候做。
3. 如果一台机器有有多个redis实例，也是手动做aof重写，并且一台完了再做另一台。不要有多台同时做。



### 2. 复制 replication
复制可以解决单点问题，将一个数据多个副本放在不同机器上。

#### 1. 命令 slave of
下面是1个master 2个slave的配置

**master配置**

```shell
port 6479
daemonize yes
logfile "logs/6479.log"
dbfilename "dump6479.rdb"
dir "/home/www/programs/redis/master-slave/data"

```
**2个slave的配置**  
```shell
port 6480
daemonize yes
logfile "logs/6480.log"
dbfilename "dump6480.rdb"
dir "/home/www/programs/redis/master-slave/data"
slaveof 127.0.0.1 6479 #指定master

```

```shell
port 6481
daemonize yes
logfile "logs/6481.log"
dbfilename "dump6481.rdb"
dir "/home/www/programs/redis/master-slave/data"
slaveof 127.0.0.1 6479

```



现在有三个redis服务
|实例|端口|
|---|---|
|master|6479|
|slave1|6480|
|slave2|6481|


**在master节点查看复制的信息**
```shell
[www@localhost master-slave]$ redis-cli -p 6479
127.0.0.1:6479> info replication
# Replication
role:master #表示是master节点
connected_slaves:2 #两个slave
slave0:ip=127.0.0.1,port=6480,state=online,offset=43,lag=0
slave1:ip=127.0.0.1,port=6481,state=online,offset=43,lag=0
master_repl_offset:43
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:2
repl_backlog_histlen:42

```

**在slave节点查看复制的信息**
```shell
127.0.0.1:6480> info replication
# Replication
role:slave #是从节点
master_host:127.0.0.1 #master ip
master_port:6479 #master port
master_link_status:up
master_last_io_seconds_ago:7
master_sync_in_progress:0
slave_repl_offset:141
slave_priority:100
slave_read_only:1
connected_slaves:0
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
127.0.0.1:6480> 

```



#### 切换主节点
下面 6480端口的redis 是6479的slave节点，现在要切换到6579这个端口的redis去
1. 注意 slaveof no one 不会删掉数据
2. 切换master后 slave会删除所有数据 

**下面是一个切换过程**

```shell
127.0.0.1:6480> slaveof no one #d断开与master的链接
OK
127.0.0.1:6480> info replication #已经不是slave了
# Replication
role:master
connected_slaves:0
master_repl_offset:4283
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
127.0.0.1:6480> keys * #数据还在
1) "c"
2) "g"
3) "i"
4) "a"
5) "e"
6) "k"
7) "aa"
127.0.0.1:6480> slaveof 127.0.0.1 6579 #master切换到 6579
OK
127.0.0.1:6480> keys * #数据已经清空了。 
(empty list or set)
127.0.0.1:6480> info replication #master已经是6579了
# Replication
role:slave
master_host:127.0.0.1
master_port:6579
master_link_status:up
master_last_io_seconds_ago:3
master_sync_in_progress:0
slave_repl_offset:29
slave_priority:100
slave_read_only:1
connected_slaves:0
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
127.0.0.1:6480> 

```

**在线上一定要特别小心**

####  减少延迟 repl-disable-tcp-nodelay
系统为了减少网络开销，有一个参数tcp-nodelay,如果开启，不管有多少数据，立即发送出去。如果关闭的话，系统会数据积累到一定量才发出去，这样延时就加大了。
```shell
repl-disable-tcp-nodelay no #表示不禁用 tcp_nodelay 也就是开启tcp_nodelay, 立即发送
```



#### 建立复制过程

1. 执行slaveof masterip port 后，slave保存主节点的信息。 但是与主节点的链接状态是down(下线)状态。
```shell
127.0.0.1:6480> info replication
# Replication
role:slave
master_host:127.0.0.1
master_port:6479
master_link_status:down #与master的链接状态是下线状态

```

2. slave 内部定时任务(每秒执行一次) 与主节点建立连接(主节点端口24555)，如果连接失败会一直重试，知道执行slaveof on one为止。
当链接成功后会看到日志

```java
21721:S 12 Sep 09:37:18.279 * Connecting to MASTER 127.0.0.1:6479
21721:S 12 Sep 09:37:18.279 * MASTER <-> SLAVE sync started

```

3. slave 发送ping命令，检查套接字是否可用，master是否可以接受处理命令 如果失败，断开连接，回到上一步的状态。
4. 权限验证
5. 同步数据集。
主节点会将所有数据发送到从节点(非常耗时)。
6. 命令持续复制。



#### 同步细节
##### 1. 偏移量
1. 主从节点都会维护自己的偏移量
2. master 节点处理完命令后会把偏移量做累加
>master_repl_offset:3697
3. slave  收到主节点发送的命令后也会累加自己的偏移量
>slave_repl_offset:491
4. slave 会定时上报自己的复制偏移量给master
> slave0:ip=127.0.0.1,port=6481,state=online,offset=3697,lag=1

**注意**
master_repl_offset - slave_offset 判断主从复制健康程度。

##### 2. Run Id
1. 每一个节点都会有一个40位的字符串作为运行id。redis不是用ip:port来区分节点的，是用runId来区分的。 
2. runid在redis重启后会改变。
如果修改了配置 比如修改hash-max-ziplist-value后，需要重新加载才能生效。可以使用**debug reload**，保持runId不变的情况下重新加载rdb。 **redbug reload 会生成新的rdb，清空数据，再加载rdb 会有较长时间的阻塞。**

##### 3。 psync (partial sync)部分复制
>psync master-runid offset
offset 是slave 当前的复制偏移量

1. slave发送psync给master。 如果是第一次offset是-1
2. master根据情况返回。 如果返回**fullresync runid offset** 从节点会全量复制。如果返回**CONTINUE** 从节点执行部分复制。

#### 全量复制
1. slave发送psync给master 主节点返回了  **fullresync runid offset** 决定全量复制。
salve的log如下
```shell
21721:S 12 Sep 09:37:18.280 * Partial resynchronization not possible (no cached master)
21721:S 12 Sep 09:37:18.333 * Full resync from master: 7e99c7f4f2897a26f8c52d6764cd0f2b13e4bd0f:491
```

2. master会执行bgsave保存rdb文件到本地
3. master发送rdb给从文件
这个过程中如果rdb非常大，发送时间操作了**rpl-timeout**，从节点会放弃接受rdb文件并清理已下载的文件，导致全量复制失败。
4. master发送rdb过程中，接受到的新的写命令放入**复制客户端缓冲区**（client-output-buffer-limit）内，当发完毕后再将缓冲区数据发送给slave。

如果在发送期间 复制客户端满了，会宣告复制失败，所以如果流量很大需要把client-output-buffer-limit调大一点。
5. salve接受完rdb文件和**复制客户端缓冲区**中的数据后，清空自己数据，并加载rdb文件。

```java
21721:S 12 Sep 09:37:18.738 * MASTER <-> SLAVE sync: receiving 56 bytes from master #接受master数据完成
21721:S 12 Sep 09:37:18.738 * MASTER <-> SLAVE sync: Flushing old data #清除自己的数据
21721:S 12 Sep 09:37:18.738 * MASTER <-> SLAVE sync: Loading DB in memory #加载master发来的rdb和复制客户端缓冲区中的数据
21721:S 12 Sep 09:37:18.738 * MASTER <-> SLAVE sync: Finished with success #加载完成

```
6. 如果加载完了 salve开启了 aof功能，会立即bgrewriteaof操作。
7. 结束
 ![部署](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/redis-fullsync.png)


**整个过程非常耗时，经验值是6G内存2分钟**




#### 部分复制(psync)
 当slave正在复制master时候，如果网络闪断，或者命令对视，从节点会要求补发丢失的命令数据。 如果主节点**复制积压缓冲区**内存在这不恩数据就可以直接发送给从节点。

 1. master和slave失去链接
 2. master响应命令并将写命令存放子啊**复制积压缓冲区**，默认1M
 3. 当主从连接恢复后，从节点将自生复制的offset和master的runid发送给master
 4. master验证了runnid与自己一致后，根据offset在自己的**复制积压缓冲区**中寻找数据，如果在缓冲区中，向从节点发送**CONTINUE**命令，表示可以进行部分复制。
 5.master根据offset把**复制积压缓冲区**中的数据发送给slave
 ![部署](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/redis-psync.png)


#### 心跳
1. master默认每10s发送ping命令给salve。通过repl-ping-slave-period来控制发送频率
2. slave默认每1秒发送replconf ack offset 上报offset给master

#### 异步复制
1. master接受到写命令执行后立即返回客户端
2. 异步发送给slave

master和slave之间会有延迟，lag=0 就是延迟的量
```shell
slave0:ip=127.0.0.1,port=6480,state=online,offset=43,lag=0
```

一篇文章
https://www.cnblogs.com/svan/p/7366397.html



### 部署的要注意的问题
#### 1. 读写分离
读写分离需要注意 master与slave的延迟，可以通过 
>master_repl_offset:26111
与
>slave0:ip=127.0.0.1,port=6481,state=online,offset=26111,lag=0
中的offset的差值来观察，当大于有个阈值的时候通知客户端，不要再读了slave了。


#### 2. 主从配置不一致
1. 主从之间有点配置是可以不一致的，比如aof 是否开启。
2. 有的配置必须一致，比如 maxmemory,hash-max-ziplist-entries等

**maxmemory不一样**:，slave小于master，如果slave的数据量超过了maxmemory，会根据maxmemory-policy进行淘汰，导致数据不一致。
**hash-max-ziplist-entries不一样**: master和salve存的数据一样，但是占用的内存会有很大的差异。

#### 3. 避免全量复制
1. 某节点不包括任何主节点信息: 会出发全量同步。 如果必须这样的话可以在低峰时候做。
2. **复制积压缓冲区**(repl_backlog_size)太小: 如果主从断开连接后，从节点再次连接发送psync offset runid请求部分复制，如果偏移量不在**复制积压缓冲区**中，
会退化为**全量复制**。
解决办法: 统计高峰时候 mster_repl_offset，计算一分钟的差值，规划一个合理的repl_backlog_size值。

#### 4.复制风暴
1. 主节点复制风暴
一台master如果连接了多个slave的话，当master重启后，多个slave会同时进行**全量复制**，可能将master的网络带宽打满。
**应对方法**：减少slave的量，一台一台地进行

2. 单机多实例复制风暴
如果一台机器上部署多个redis的master实例，如果这台机器出现网络故障，等ta恢复的时候，会有大量从节点针对这台机器进行全量复制，可能将磁盘或者网络带宽打满。
**应对方法**:master尽量不要部署在同一台机器上。 或者 主节点故障后提供故障转移机制，避免恢复后全量复制。

