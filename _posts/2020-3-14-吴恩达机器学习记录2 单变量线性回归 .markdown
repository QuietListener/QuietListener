---
layout: post
title: 吴恩达机器学习记录2 单变量线性回归
date: 2020-3-14 14:32:00
categories:  机器学习
---
<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 1. 机器学习的过程
1. 提供训练数据集(traning set)
2. 使用 学习算法(learning algorithm) 找到 hypothesis（一个函数）
3. 使用 hypothesis 对新数据进行预测。

例如预测房价  
1. 训练集为(面积，房价)，例如 (100,100),(120,118) 等
2. 学习算法是 梯度下降
3. 得到的hypothesis是 h = a+bx;

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200314-hypothesis.jpg" width="400"> 


# 2. 房价预测模型

## 1. 训练数据 (面积，房价)

面积 | 价格
-|-
2104 | 600
1416 | 232
1534 | 315


## 2. (假设)hypothesis
h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x

任务是找到  合适的 θ<sub>0</sub> 和 θ<sub>1</sub>  来预测新的数据。

## 3. 通过算法找到得到合适的 θ<sub>0</sub> 和 θ<sub>1</sub>
### 1. 代价函数
1. 训练数据是： (x<sup>(i)</sup>,y<sup>(i)</sup>)    有n个。
  
2. 预测函数式： $$ h_θ(x^{(i)} )$$  

3.  参数有：$$ θ_0 ,θ_1 $$  

4.  代价函数：$$J(θ_0,θ_1) = \frac{1}{2n}\sum_{i=1}^n(h_θ(x^{(i)} - y^{(i)} )^2 $$ 

    上线这个叫**平方误差代价函数**

5. 目的： 最小化:$$J(θ_0,θ_1)$$


**注意  $$h_θ(x)$$ 是关于 x的函数 , $$J(θ_0,θ_1)$$ 是关于 $$θ_0,θ_1$$ 的函数.**


### 2. 使用等高线理解最小化 $$J(θ_0,θ_1)$$ 的过程
1. 
<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression1.jpg" width="600"> 

2. 
<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression2.jpg" width="600">

3. 
<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression3.jpg" width="600"> 

### 3. 梯度下降算法(Gradient descent)来找 $$J(θ_0,θ_1) $$ 最小值

#### 1.  基本思想
1. 目标： 找到 $$J(θ_0,θ_1) $$ 最小值
2. 过程: 
   1. 从某个  $$(θ_0,θ_1) $$ 开始
   2. 不断改变  $$(θ_0,θ_1) $$ 的值，直到最后达到一个最小值。   

   <img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression4.jpg" width="400"> 

#### 2. 算法
不断更新  $$(θ_0,θ_1) $$ 知道找到局部最小值，如下图所示，


   <img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression5.jpg" width="400"> 
   

   其中 α 代表 梯度下降的速率，也就是每一步走多长。**α 太小 收敛速度慢 α 太大，可能不收敛甚至 发散。** 
   如下图
  
  <img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression6.jpg" width="400"> 

  

### 4. 线性回归 的 梯度下降 算法   


$$ \frac{\partial J(θ_0,θ_1) }{\partial θ_j}  = \frac{\partial }{\partial θ_j}  \frac{1}{2n}\sum_{i=1}^n(h_θ(x^{(i)}) - y^{(i)} )^2  $$ 


$$ \frac{\partial J(θ_0,θ_1) }{\partial θ_0} =   \frac{1}{n}\sum_{i=1}^n(h_θ(x^{(i)}) - y^{(i)} ) $$ 

$$ \frac{\partial J(θ_0,θ_1) }{\partial θ_1} =   \frac{1}{n}\sum_{i=1}^n(h_θ(x^{(i)}) - y^{(i)} )* x^{(i)} $$ 


**所以 $θ_0,θ_1$可以用下面的公式进行迭代**    

$$θ_0 = θ_0 - α \frac{\partial J(θ_0,θ_1) }{\partial θ_0} =   θ_0- α \frac{1}{n}\sum_{i=1}^n(h_θ(x^{(i)}) - y^{(i)} ) $$ 

$$θ_1 = θ_1 - α  \frac{\partial J(θ_0,θ_1) }{\partial θ_1} =  θ_1 - α \frac{1}{n}\sum_{i=1}^n(h_θ(x^{(i)}) - y^{(i)} )* x^{(i)}$$ 

上面的方式叫 **批量梯度下降**（ **Batch Gradient Descent** ）每次迭代 都需要把所有训练数据集 算一遍 (因为有 $$\sum_{i=1}^n$$)， 效率低。
