---
layout: post
title: 吴恩达机器学习记录2 单变量线性回归
date: 2020-3-14 14:32:00
categories:  机器学习
---


<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 1. 机器学习的过程
1. 提供训练数据集(traning set)
2. 使用 学习算法(learning algorithm) 找到 hypothesis（一个函数）
3. 使用 hypothesis 对新数据进行预测。

例如预测房价  
1. 训练集为(面积，房价)， (100,100),(120,118) 等
2. 学习算法是 梯度下降
3. 得到的hypothesis是 h = a+bx;

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200314-hypothesis.jpg" width="400"> 


# 房价预测模型

## 1. 训练数据 (面积，房价)

面积 | 价格
-|-
2104 | 600
1416 | 232
1534 | 315

## 2. hypothesis
h<sub>θ</sub>(x) = θ<sub>0</sub> + θ<sub>1</sub>x

任务是找到  合适的 θ<sub>0</sub> 和 θ<sub>1</sub>  来预测新的数据。

## 3. 怎么得到合适的 θ<sub>0</sub> 和 θ<sub>1</sub>
### 1. 代价函数
1. 训练数据是： (x<sup>(i)</sup>,y<sup>(i)</sup>)    有n个。
  
2. 预测函数式： \( h_θ(x^{(i)}) \)  

3.  参数有：\( θ_0 ,θ_1 \)   

4.  代价函数：\( J(θ_0,θ_1) = \frac{1}{2n}\sum_{i=1}^n(h_θ(x^{(i)} - y^{(i)} )^2 \) 

    上线这个叫平方误差代价函数

5. 目的： 最小化:\( J(θ_0,θ_1) \)


**注意  \(h_θ(x)\)是关于 x的函数 \( J(θ_0,θ_1) \) 是关于 \(θ_0,θ_1\) 的函数.**



### 2. 使用等高线 理解 最小化 \( J(θ_0,θ_1) \) 的过程
1. 
<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression1.jpg" width="400"> 

2. 
<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression2.jpg" width="400">

3. 
<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression3.jpg" width="400"> 

### 3. 使用梯度下降(Gradient descent)来找 \( J(θ_0,θ_1) \) 最小值

#### 1.  基本思想
1. 目标： 找到 \( J(θ_0,θ_1) \) 最小值
2. 过程: 
   1. 从某个  \( (θ_0,θ_1) \) 开始
   2. 不断改变  \( (θ_0,θ_1) \) 的值，直到最后达到一个最小值。
   <img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression4.jpg" width="400"> 

#### 2. 算法
不断更新  \( (θ_0,θ_1) \) 知道找到局部最小值，如下图所示，


   <img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200316-regression5.jpg" width="400"> 
   

   其中 α 代表 梯度下降的速率，也就是每一步走多长。




  


