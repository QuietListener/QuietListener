---
layout: post
title: 吴恩达机器学习记录4 多元线性回归
date: 2020-3-17 14:32:00
categories:  机器学习
---

<script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 1. what
## 1. 有多个变量(以预测房屋价格为例)
<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200317-lr1.jpg" width="600"> 


## 2. 假设空间
<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200317-lr2.jpg" width="600"> 

假设空间可以写成两个向量的乘积 
$$ h_θ(x) = θ^T X $$

# 2. 多元变量的梯度下降
<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200317-lr3.jpg" width="600"> 

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200317-lr4.jpg" width="600"> 


具体一点就是:

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200317-lr5.jpg" width="400"> 


# 3. 特征缩放(feature scaling)，使得梯度下降收敛速度。
比如 $$0<x_1<100 ,   -2 < x_2<0.5 $$
这样可能使得梯度下降很缓慢，因为有的变量范围很广。如果进行缩放将 特性的范围控制在  $$ -3<x_1< 3 $$ 之内，会加快收敛速度。

比如 可以使用均值
$$  x_1 = \frac{x_1 - 50}{100} $$  先在 $$ -0.5 <x_1 < 0.5 $$

# 4. 如何选择 学习率: α
 **1. 学习率过大会不收敛，或者 震荡**
 **2. 学习率太小 收敛速度会非常慢。**

如下图 左边是学习率太小， 右边是学习率太大

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200317-lr6.jpg" width="600"> 


**Andrew NG的建议是: 尝试 多个α值: 0.001, 0.003, 0.01,0.03 0.1, 0.3, 1**

**最终是达到一个速度和精度 的平衡。**

# 5. 特征(feature)选择
有时候**自己定义一个新特征**，会得到一个更好的模型。   

比如下图，本来特征是 房屋的 **长和宽**，我们使用**面积**来作为特性。
<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200317-lr7.jpg" width="400"> 



# 6. 正规方程(normal equation)，直接求出 $$J(θ)$$ 最小值时候的 θ值
## 1. 基本思想

$$J(θ)$$ 是一个 凸函数，只有一个最小值，根据微分的知识，**$$J(θ)$$在最小值时候它的微分等于0**。 这样就可以直接求出 $$J(θ)$$ 取最小值时候的 θ 值。

例如下面假设 θ 是一个实数(不是向量)。当设导数为0，就能求出 θ的值。

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200317-lr8.jpg" width="400"> 


**同样，对于多变量的$$J(θ)$$，每个变量的偏导数都为0，也可以直接求出 θ**，

也有公式可直接应用如下所示。  

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200317-lr9.jpg" width="400"> 


## 2. 梯度下降和正规方程的区别

<img src="https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/20200317-lr10.jpg" width="400"> 

**Andrew Ng的建议:** 一般当特性数量小于10000 直接使用正规方程，如果n太大，还是使用梯度下降。

# 7. 正规方程时候遇到 矩阵 不可逆的情况
$$ θ = (X^TX)^{-1} $$ 如果$$ (X^TX)^{-1} $$不可逆 θ就不能求出来。
## 1. 原因:
### 1. 有冗余的feature  
比如有个参数是 面积的平方 单位是 平方米，还有一个也是面积的平方，是平方厘米。
### 2. 训练数据比  参数个数还少
比如有100个参数，但是只有10个训练数据。
**解决办法：可以删除一些参数或者 使用正则化**


