---
layout: post
title: redis运维 备忘
date: 2020-9-9 14:32:00
categories:  java maven
---
### 1. 持久化 RDB AOF
#### 1. redis持久化的机制 
redis有RDB和AOF两种持久化机制，下次重启时候利用之前持久化的文件即可恢复数据。

#### 2.RDB
RDB是把当前**进程数据生成快照保存到硬盘**的过程，可以手动也可以自动触发。

##### 1.手动rdb
```shell
127.0.0.1:6379> set aabb ccdd
OK
127.0.0.1:6379> bgsave
Background saving started

```

log 信息
```java
8906:M 10 Sep 10:03:38.417 * Background saving started by pid 8995
8995:C 10 Sep 10:03:38.568 * DB saved on disk
8995:C 10 Sep 10:03:38.568 * RDB: 6 MB of memory used by copy-on-write
8906:M 10 Sep 10:03:38.643 * Background saving terminated with success

```


##### 2.自动rdb
```shell
#rdb 10秒钟有3次修改就作rdb
save 10 3
```

执行三次set   

```shell
127.0.0.1:6379> 
127.0.0.1:6379> set aabb ccdd
OK
127.0.0.1:6379> set aabb ccdd1
OK
127.0.0.1:6379> set aabb ccdd12
OK
127.0.0.1:6379> set aabb ccdd124
OK
```

log信息   

```java
9184:M 10 Sep 10:13:03.227 * Background saving started by pid 9195
9195:C 10 Sep 10:13:03.240 * DB saved on disk
9195:C 10 Sep 10:13:03.242 * RDB: 0 MB of memory used by copy-on-write
9184:M 10 Sep 10:13:03.328 * Background saving terminated with success

```

##### 3. rdb过程
rdb是使用fork一个子进程在后台执行的。
1. 判断是否已经有rdb/aof子进程，如果存在就返回。
2. fork一个子进程，fork会阻塞父进程
3. 父进程fork完成后，bgsave返回Background saving stated.
4. 子进程根据父进程内存生成临时快照文件，并对原文件做快照替换。
5. 进程发送信号给父进程表示完成。


下面是状态。
```shell
127.0.0.1:6379> info stats
# Stats
total_connections_received:6
total_commands_processed:100007
instantaneous_ops_per_sec:0
total_net_input_bytes:10089094
total_net_output_bytes:504551
instantaneous_input_kbps:0.00
instantaneous_output_kbps:0.00
rejected_connections:0
sync_full:0
sync_partial_ok:0
sync_partial_err:0
expired_keys:0
evicted_keys:51083
keyspace_hits:0
keyspace_misses:0
pubsub_channels:0
pubsub_patterns:0
latest_fork_usec:322  #上一次fork 使用了 322微妙(阻塞了父进程322微秒)
migrate_cached_sockets:0
127.0.0.1:6379> 

```


```shell
127.0.0.1:6379> info Persistence
# Persistence
loading:0
rdb_changes_since_last_save:0
rdb_bgsave_in_progress:0
rdb_last_save_time:1599708110 #上次开始rdb的时间
rdb_last_bgsave_status:ok
rdb_last_bgsave_time_sec:0
rdb_current_bgsave_time_sec:-1
aof_enabled:0
aof_rewrite_in_progress:0
aof_rewrite_scheduled:0
aof_last_rewrite_time_sec:-1
aof_current_rewrite_time_sec:-1
aof_last_bgrewrite_status:ok
aof_last_write_status:ok
127.0.0.1:6379> 
```

##### 4 rdb优缺点
###### 优点
1. rdb是**压缩**过的二进制文件，是一个时间点的快照。适用于全量备份，全量复制等场景，用于灾难恢复。
2. redis加载rdb也远远快于AOF

###### 缺点
1. 无法实时的持久化，或者秒级持久化。
bgsave需要fork进程，非常耗时，线上8G内存的redis实例，fork需要200毫秒。如果一个qps是5万的话，就影响了1/5，大约1万个请求。


#### 3.AOF(append of file)
##### 1. 原理
aof以独立日志的方式记录每次写命令，重启时候在重新执行aof文件中的命令达到恢复目的。
**aof解决了吃鸡化的实时性**

#### 2#. aof工作流程
1. 写入命令写到aof_buf
2. aof缓冲根据**策略**向硬盘做**同步**操作
3. aof文件越来越大，需要定期对aof重写，达到压缩的目的
4. redis重启时候，可以加在aof文件进行数据恢复

##### 注意事项
1. aof使用文本协议格式
有很好的兼容性和可读性。
2. aof不直接写入磁盘，使用aof_buf 缓冲区
单线程，防止io阻塞，可以有不同的策略将缓冲同步到磁盘，在性能和安全性做出平衡。

##### 3. aof缓冲区同步策略
1. alaways 命令写aof_buf后调用fsync同步到磁盘文件，然后再返回。
2. everysec 命令写入aof_buf后调用write操作，立即返回，由一个单独的同步线程每秒做一次fsync同步。
3. no 命令写入aof_buf后调用write操作，立即返回，不对aof文件做fsync同步。

**建议使用everysec**，如果突然宕机，值丢失1秒左右的数据。

##### 4. aof文件重写
命令不断写入 aof会越来越大，重写机制可以解决这个问题。 
aof文件变小了 redis加载aof文件也会更快。

###### 1. 为什么重写可以使得命令变小
1. 进程内超时的数据可以不用写了。
2. 多条命令可以合并，比如 lpush l a , lpush l b 可以合并为 lpush l a b
3. 旧的aof文件里的无效命令已经可以不用写了。

#### aof重写触发时机
##### 1. 手动触发
bgrewriteaof命令

##### 2. 自动触发
```shell
#aof
appendonly yes
appendfilename "aof6379.aof"
#重写
#同时满足下面2个才进行重写
#当aof文件大于64M才重写
auto-aof-rewrite-min-size 40M
#当aof比上一次aof后的文件大的比例，比如上次是100m percentage设置为20  120M的时候就可以进行重写了。
auto-aof-rewrite-percentage  20

```


下面是自动开始aof rewrite的过程
```shell
13195:M 10 Sep 14:41:23.925 * Starting automatic rewriting of AOF on 6405058400% growth
13195:M 10 Sep 14:41:23.926 * Background append only file rewriting started by pid 13404
13195:M 10 Sep 14:41:25.071 * AOF rewrite child asks to stop sending diffs.
13404:C 10 Sep 14:41:25.071 * Parent agreed to stop sending diffs. Finalizing AOF...
13404:C 10 Sep 14:41:25.071 * Concatenating 0.48 MB of AOF diff received from parent.
13404:C 10 Sep 14:41:25.098 * SYNC append only file rewrite performed
13404:C 10 Sep 14:41:25.099 * AOF rewrite: 9 MB of memory used by copy-on-write #在fork后 又有9M的内存修改了，所以有9M的内存做了复制
13195:M 10 Sep 14:41:25.214 * Background AOF rewrite terminated with success
13195:M 10 Sep 14:41:25.214 * Residual parent diff successfully flushed to the rewritten AOF (0.62 MB)
13195:M 10 Sep 14:41:25.215 * Background AOF rewrite finished successfully

```

重写aof文件后问价大小的对比,**aof文件从60多M变为了13M**
```shell
[www@localhost tests]$ ls data -lah
总用量 71M
drwxrwxr-x. 2 www www   43 9月  10 14:41 .
drwxrwxr-x. 4 www www   65 9月  10 14:40 ..
-rw-r--r--. 1 www www  61M 9月  10 14:41 aof6379.aof
-rw-rw-r--. 1 www www 5.5M 9月  10 14:41 dump6379.rdb
[www@localhost tests]$ ls data -lah
总用量 19M
drwxrwxr-x. 2 www www   43 9月  10 14:41 .
drwxrwxr-x. 4 www www   65 9月  10 14:40 ..
-rw-rw-r--. 1 www www  13M 9月  10 14:41 aof6379.aof
-rw-rw-r--. 1 www www 2.8M 9月  10 14:41 dump6379.rdb
```

##### 3. aof rewrite的过程
1. 如果正在执行aof 直接返回
2. 如果正在执行bgsave要等到bgsave完成后再执行 aof重写
3. 父进程fock子进程，与bgsave开销一样。
4. 1 主进程继续响应命令，将写命令写入 aof缓冲区，并根据策略做同步处理
4. 2 子进程只能看到fork时候的内存快照，fock后父进程响应的写命令修改的这些新数据，子进程看不见。 redis使用 aof重写缓冲区来保存这部分新数据。防止数据丢失。
5. 子进程根据内存快照，按照命令合并等规则写入新的aof文件。每次写入aof-rewrite-incremental-fsync 字节(默认32M)的数据。防止磁盘阻塞
6. 1 先的aof文件写入后，发送信号给父进程。
6. 2 父进程把aof重写缓冲区的数据写入到新的aof文件。
6. 3 替换老的aof文件。


### 重启后持久化文件加载
 优先加载aof
 ![部署](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/redis-aof-rdb-reload.png) 


### 优化的地方
#### 1. fork
##### 问题
1. aof或者rdb都需要操作系统fork操作。fork是很重的一个操作。
2. fork时候不需要复制父进程的内存空间，但要复制父进程的**内存页**，10G内存也需要复制20Md内存页。
3. 经验值是10G内存大概需要20毫秒fork时间。


##### 怎么优化
1. 单个redis实例最好不要操作10G内存。
2. redis尽量使用物理机，使用用fork很高效的虚拟机。不用虚拟机。
3. 降低fork频率，不使用自动rdb，aof 合理配置 减少不必要的全量复制。


### rdb或者aof重写时的 cpu 内存 磁盘
#### 1.cpu和磁盘
子进程在重写或者rdb时候，将进程内的数据粉笔写入文件，这个过程视乎cpu密集型，也是io密集型。
**优化**
1. 一台机器上如果有多个redis实例，最好保证同一时刻只有一个实例在做rdb或者aof重写操作。
2. 每一个redis实例，将aof/rdb文件放在不同的磁盘。
3. no-appendfsync-on-rewrite yes 这样也有了丢失整个aof重写期间的数据的风险。


#### 2. 内存
linux有copy-on-write机制，当父进程处理写请求时候会将要修改的内存页创建副本。这是一部分开销。

一篇文章
https://cloud.tencent.com/developer/article/1633077

#### 3 aof追加阻塞
主要是磁盘io问题

### 部署
1. 不要在同一台机器上部署io密集型的服务。例如 kafka
2. aof重写不要自动执行，可以外部监控，然后在适当的时候手动做aof重写，比如凌晨或者业务低风时候做。
3. 如果一台机器有有多个redis实例，也是手动做aof重写，并且一台完了再做另一台。不要有多台同时做。



### 2. 复制 replication
复制可以解决单点问题，将一个数据多个副本放在不同机器上。

#### 1. 命令 slave of
下面是1个master 2个slave的配置

**master配置**

```shell
port 6479
daemonize yes
logfile "logs/6479.log"
dbfilename "dump6479.rdb"
dir "/home/www/programs/redis/master-slave/data"

```
**2个slave的配置**  
```shell
port 6480
daemonize yes
logfile "logs/6480.log"
dbfilename "dump6480.rdb"
dir "/home/www/programs/redis/master-slave/data"
slaveof 127.0.0.1 6479 #指定master

```

```shell
port 6481
daemonize yes
logfile "logs/6481.log"
dbfilename "dump6481.rdb"
dir "/home/www/programs/redis/master-slave/data"
slaveof 127.0.0.1 6479

```



现在有三个redis服务

|实例|端口|
|---|---|
|master|6479|
|slave1|6480|
|slave2|6481|


**在master节点查看复制的信息**
```shell
[www@localhost master-slave]$ redis-cli -p 6479
127.0.0.1:6479> info replication
# Replication
role:master #表示是master节点
connected_slaves:2 #两个slave
slave0:ip=127.0.0.1,port=6480,state=online,offset=43,lag=0
slave1:ip=127.0.0.1,port=6481,state=online,offset=43,lag=0
master_repl_offset:43
repl_backlog_active:1
repl_backlog_size:1048576
repl_backlog_first_byte_offset:2
repl_backlog_histlen:42

```

**在slave节点查看复制的信息**
```shell
127.0.0.1:6480> info replication
# Replication
role:slave #是从节点
master_host:127.0.0.1 #master ip
master_port:6479 #master port
master_link_status:up
master_last_io_seconds_ago:7
master_sync_in_progress:0
slave_repl_offset:141
slave_priority:100
slave_read_only:1
connected_slaves:0
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
127.0.0.1:6480> 

```



#### 切换主节点
下面 6480端口的redis 是6479的slave节点，现在要切换到6579这个端口的redis去
1. 注意 slaveof no one 不会删掉数据
2. 切换master后 slave会删除所有数据 

**下面是一个切换过程**

```shell
127.0.0.1:6480> slaveof no one #d断开与master的链接
OK
127.0.0.1:6480> info replication #已经不是slave了
# Replication
role:master
connected_slaves:0
master_repl_offset:4283
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
127.0.0.1:6480> keys * #数据还在
1) "c"
2) "g"
3) "i"
4) "a"
5) "e"
6) "k"
7) "aa"
127.0.0.1:6480> slaveof 127.0.0.1 6579 #master切换到 6579
OK
127.0.0.1:6480> keys * #数据已经清空了。 
(empty list or set)
127.0.0.1:6480> info replication #master已经是6579了
# Replication
role:slave
master_host:127.0.0.1
master_port:6579
master_link_status:up
master_last_io_seconds_ago:3
master_sync_in_progress:0
slave_repl_offset:29
slave_priority:100
slave_read_only:1
connected_slaves:0
master_repl_offset:0
repl_backlog_active:0
repl_backlog_size:1048576
repl_backlog_first_byte_offset:0
repl_backlog_histlen:0
127.0.0.1:6480> 

```

**在线上一定要特别小心**

####  减少延迟 repl-disable-tcp-nodelay
系统为了减少网络开销，有一个参数tcp-nodelay,如果开启，不管有多少数据，立即发送出去。如果关闭的话，系统会数据积累到一定量才发出去，这样延时就加大了。

```shell
repl-disable-tcp-nodelay no #表示不禁用 tcp_nodelay 也就是开启tcp_nodelay, 立即发送
```



#### 建立复制过程

1. 执行slaveof masterip port 后，slave保存主节点的信息。 但是与主节点的链接状态是down(下线)状态。

```shell
127.0.0.1:6480> info replication
 # Replication
role:slave
master_host:127.0.0.1
master_port:6479
master_link_status:down #与master的链接状态是下线状态
```

2. slave 内部定时任务(每秒执行一次) 与主节点建立连接(主节点端口24555)，如果连接失败会一直重试，知道执行slaveof on one为止。
当链接成功后会看到日志

```java
21721:S 12 Sep 09:37:18.279 * Connecting to MASTER 127.0.0.1:6479
21721:S 12 Sep 09:37:18.279 * MASTER <-> SLAVE sync started

```

3. slave 发送ping命令，检查套接字是否可用，master是否可以接受处理命令 如果失败，断开连接，回到上一步的状态。
4. 权限验证
5. 同步数据集。
主节点会将所有数据发送到从节点(非常耗时)。
6. 命令持续复制。



#### 同步细节
##### 1. 偏移量
1. 主从节点都会维护自己的偏移量
2. master 节点处理完命令后会把偏移量做累加
>master_repl_offset:3697
3. slave  收到主节点发送的命令后也会累加自己的偏移量
>slave_repl_offset:491
4. slave 会定时上报自己的复制偏移量给master
> slave0:ip=127.0.0.1,port=6481,state=online,offset=3697,lag=1

**注意**
master_repl_offset - slave_offset 判断主从复制健康程度。

##### 2. Run Id
1. 每一个节点都会有一个40位的字符串作为运行id。redis不是用ip:port来区分节点的，是用runId来区分的。 
2. runid在redis重启后会改变。
如果修改了配置 比如修改hash-max-ziplist-value后，需要重新加载才能生效。可以使用**debug reload**，保持runId不变的情况下重新加载rdb。 **redbug reload 会生成新的rdb，清空数据，再加载rdb 会有较长时间的阻塞。**

##### 3。 psync (partial sync)部分复制
>psync master-runid offset
offset 是slave 当前的复制偏移量

1. slave发送psync给master。 如果是第一次offset是-1
2. master根据情况返回。 如果返回**fullresync runid offset** 从节点会全量复制。如果返回**CONTINUE** 从节点执行部分复制。

#### 全量复制
1. slave发送psync给master 主节点返回了  **fullresync runid offset** 决定全量复制。
salve的log如下
```shell
21721:S 12 Sep 09:37:18.280 * Partial resynchronization not possible (no cached master)
21721:S 12 Sep 09:37:18.333 * Full resync from master: 7e99c7f4f2897a26f8c52d6764cd0f2b13e4bd0f:491
```

2. master会执行bgsave保存rdb文件到本地
3. master发送rdb给从文件
这个过程中如果rdb非常大，发送时间操作了**rpl-timeout**，从节点会放弃接受rdb文件并清理已下载的文件，导致全量复制失败。
4. master发送rdb过程中，接受到的新的写命令放入**复制客户端缓冲区**（client-output-buffer-limit）内，当发完毕后再将缓冲区数据发送给slave。

如果在发送期间 复制客户端满了，会宣告复制失败，所以如果流量很大需要把client-output-buffer-limit调大一点。
5. salve接受完rdb文件和**复制客户端缓冲区**中的数据后，清空自己数据，并加载rdb文件。

```java
21721:S 12 Sep 09:37:18.738 * MASTER <-> SLAVE sync: receiving 56 bytes from master #接受master数据完成
21721:S 12 Sep 09:37:18.738 * MASTER <-> SLAVE sync: Flushing old data #清除自己的数据
21721:S 12 Sep 09:37:18.738 * MASTER <-> SLAVE sync: Loading DB in memory #加载master发来的rdb和复制客户端缓冲区中的数据
21721:S 12 Sep 09:37:18.738 * MASTER <-> SLAVE sync: Finished with success #加载完成

```
6. 如果加载完了 salve开启了 aof功能，会立即bgrewriteaof操作。
7. 结束
 ![部署](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/redis-fullsync.png)


**整个过程非常耗时，经验值是6G内存2分钟**




#### 部分复制(psync)
 当slave正在复制master时候，如果网络闪断，或者命令对视，从节点会要求补发丢失的命令数据。 如果主节点**复制积压缓冲区**内存在这不恩数据就可以直接发送给从节点。

 1. master和slave失去链接
 2. master响应命令并将写命令存放子啊**复制积压缓冲区**，默认1M
 3. 当主从连接恢复后，从节点将自生复制的offset和master的runid发送给master
 4. master验证了runnid与自己一致后，根据offset在自己的**复制积压缓冲区**中寻找数据，如果在缓冲区中，向从节点发送**CONTINUE**命令，表示可以进行部分复制。
 5.master根据offset把**复制积压缓冲区**中的数据发送给slave
 ![部署](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/redis-psync.png)


#### 心跳
1. master默认每10s发送ping命令给salve。通过repl-ping-slave-period来控制发送频率
2. slave默认每1秒发送replconf ack offset 上报offset给master

#### 异步复制
1. master接受到写命令执行后立即返回客户端
2. 异步发送给slave

master和slave之间会有延迟，lag=0 就是延迟的量
```shell
slave0:ip=127.0.0.1,port=6480,state=online,offset=43,lag=0
```

一篇文章
https://www.cnblogs.com/svan/p/7366397.html



### 部署的要注意的问题
#### 1. 读写分离
读写分离需要注意 master与slave的延迟，可以通过 
>master_repl_offset:26111
与
>slave0:ip=127.0.0.1,port=6481,state=online,offset=26111,lag=0
中的offset的差值来观察，当大于有个阈值的时候通知客户端，不要再读了slave了。


#### 2. 主从配置不一致
1. 主从之间有点配置是可以不一致的，比如aof 是否开启。
2. 有的配置必须一致，比如 maxmemory,hash-max-ziplist-entries等

**maxmemory不一样**:，slave小于master，如果slave的数据量超过了maxmemory，会根据maxmemory-policy进行淘汰，导致数据不一致。
**hash-max-ziplist-entries不一样**: master和salve存的数据一样，但是占用的内存会有很大的差异。

#### 3. 避免全量复制
1. 某节点不包括任何主节点信息: 会出发全量同步。 如果必须这样的话可以在低峰时候做。
2. **复制积压缓冲区**(repl_backlog_size)太小: 如果主从断开连接后，从节点再次连接发送psync offset runid请求部分复制，如果偏移量不在**复制积压缓冲区**中，
会退化为**全量复制**。
解决办法: 统计高峰时候 mster_repl_offset，计算一分钟的差值，规划一个合理的repl_backlog_size值。

#### 4.复制风暴
1. 主节点复制风暴
一台master如果连接了多个slave的话，当master重启后，多个slave会同时进行**全量复制**，可能将master的网络带宽打满。
**应对方法**：减少slave的量，一台一台地进行

2. 单机多实例复制风暴
如果一台机器上部署多个redis的master实例，如果这台机器出现网络故障，等ta恢复的时候，会有大量从节点针对这台机器进行全量复制，可能将磁盘或者网络带宽打满。
**应对方法**:master尽量不要部署在同一台机器上。 或者 主节点故障后提供故障转移机制，避免恢复后全量复制。




### 3. 阻塞的问题

单线程的redis最怕的就是阻塞

#### 1. 使用的api不合理
例如大量使用O(n)时间复制读的命令
例如hgetall，可以使用hmge代替

可以查看slowlog slowlog get {n}发现慢查询

#### 2. 大对象
> redis-cli -h ip -p port bigkeys 

可以扫描大对象

#### 3.cpu饱和
redis是单线程，是cpu密集型。
例如ziplist的复杂度是o(n)到o(n平方)之间，但是省内存。如果过度要求省内存，把hash-max-ziplist-entries 调得很高的话，有可能会过多消耗cpu。导致cpu饱和

可以用下面的命令查看每秒处理的命令个数
**如果每秒处理命令个数不多，但是cpu却很高的话，就可能是上面的问题**
```shell  
[www@localhost master-slave]$ redis-cli --stat -p 6479
------- data ------ --------------------- load -------------------- - child -
keys       mem      clients blocked requests            connections          
7          1.53M    1       0       82377 (+0)          13          
7          1.53M    1       0       82380 (+3)          13          
7          1.53M    1       0       82383 (+3)          13          
7          1.53M    1       0       82386 (+3)          13          
7          1.53M    1       0       82389 (+3)          13          
7          1.53M    1       0       82392 (+3)          13  
```

#### 4 rdb或者aof fork时候柱塞
```shell
127.0.0.1:6479> info stats
# Stats
total_connections_received:14
total_commands_processed:83398
instantaneous_ops_per_sec:1
total_net_input_bytes:3152585
total_net_output_bytes:135388
instantaneous_input_kbps:0.07
instantaneous_output_kbps:0.00
rejected_connections:0
sync_full:3
sync_partial_ok:6
sync_partial_err:0
expired_keys:0
evicted_keys:0
keyspace_hits:0
keyspace_misses:0
pubsub_channels:0
pubsub_patterns:0
latest_fork_usec:26191 #上次fork使用时间 微妙
migrate_cached_sockets:0
127.0.0.1:6479> 

```

避免使用过大的实例


#### 5 AOF刷盘

看这里
https://dbawsp.com/2010.html


redis在aof持久化，会调用fsync刷盘。如果主线程发现距离上一次的fsync成功超过了2秒，为了数据安全**回阻塞主线程知道fsync执行完**。
出现这种情况多半是 aofrewrite导致磁盘io被打满了。
会看到下面这种日志
```log

3921:S 24 Jul 21:01:02.081 * Asynchronous AOF fsync is taking too long (disk is busy?). Writing the AOF buffer without waiting for fsync to complete, this may slow down Redis.
3921:S 24 Jul 21:01:04.084 * Asynchronous AOF fsync is taking too long (disk is busy?). Writing the AOF buffer without waiting for fsync to complete, this may slow down Redis.


```
**可以将no-appendfsync-on-rewrite 设置为 yes，在aofrewrite的时候，aof不调用fsync**，危险的是如果宕机，可能丢失数据。

```shell
auto-aof-rewrite-percentage               100
auto-aof-rewrite-min-size                 1gb
no-appendfsync-on-rewrite                 yes   
```


#### 6 cpu竞争
1. 与其他cpu密集型的服务竞争
2. 如果redis绑定到某个cpu上，子进程可能会与主进程竞争cpu

#### 6.内存不足或者maxmemory设置太大导致使用swap

可以用下面命令查看 redis的swap使用情况
```shell
[www@localhost master-slave]$ cat /proc/21720/smaps | grep Swap
Swap:                  0 kB
Swap:                  0 kB
Swap:                  0 kB
Swap:                  0 kB
Swap:                  0 kB
Swap:                  0 kB
Swap:                  0 kB
Swap:                  0 kB
Swap:                  0 kB
Swap:                  0 kB
Swap:                  0 kB


```

如果有较多的大于0的使用，需要优化配置了。

#### 7网络问题
1. 网络闪断  
  避免redis客户端与redis服务器跨机房

2.  redis拒绝连接.  

#默认与客户端连接数，大于这个数，就会拒绝连接。
maxclients 10000 决定了连接上限。

可以看rejected_connections知道拒绝了多少个连接

```java
127.0.0.1:6479> info stats
# Stats
total_connections_received:16
total_commands_processed:88240
instantaneous_ops_per_sec:1
total_net_input_bytes:3336550
total_net_output_bytes:143479
instantaneous_input_kbps:0.05
instantaneous_output_kbps:0.00
rejected_connections:0  //拒绝连接的个数
sync_full:3
sync_partial_ok:6
sync_partial_err:0
expired_keys:0
evicted_keys:0
keyspace_hits:0
keyspace_misses:0
pubsub_channels:0
pubsub_patterns:0
latest_fork_usec:26191
migrate_cached_sockets:0

```

3. redis没有主动关闭无效连接
如果有客户端平凡的启动或销毁，**redis默认不会关闭无效的链接**，这回导致redis连接快速消耗。
可以用下面参数主动关闭redis无效链接。

```shell
## 下面是reids主动关闭无效连接的配置
#空闲的client多久会被关闭
timeout: 600

#redis主动向空闲客户端发起ack请求，判断连接有没有效果
tcp-keepalive yes

## end 

```

4. linux系统的 进程最大打开文件数限制和tcp端口backlog限制
**linux系统每个进程对打开文件数有限制 一般是1024，可以增大这个值到65535**

```shell
[www@localhost master-slave]$ ulimit -n
1024
[www@localhost master-slave]$ su root
密码：
[root@localhost master-slave]# ulimit -n 65535  
```

**tcp 端口也有backlog限制**
socket编程中listen有一个参数。backlog是tcp请求队列的长度
int listen(int sockfd, int backlog);

真正的长度其实是  min(/proc/sys/net/core/somaxconn ,backlog).
所以要改的话需要执行
```shell
[root@localhost master-slave]# echo 511 > /proc/sys/net/core/somaxconn
```

```shell
#backlog大小，还需要调整 /proc/sys/net/core/somaxconn ，系统最终取二者较小的一个
#[root@localhost master-slave]# echo 511 > /proc/sys/net/core/somaxconn
tcp-backlog 511

```



### 3. 内存

#### 1.一些指标
```shell
[www@localhost master-slave]$ redis-cli -p 6479
127.0.0.1:6479> info memory
# Memory
used_memory:1598128
used_memory_human:1.52M #redis内存分配器分配内存总量
used_memory_rss:8118272 #从操作系统看redis进程内存占用物理内存总量
used_memory_peak:1635696 
used_memory_peak_human:1.56M #内存使用最大值，used_memory使用的峰值
used_memory_lua:36864
mem_fragmentation_ratio:5.08 #used_memory_rss/used_memory 内存碎片率
mem_allocator:jemalloc-3.6.0

```

比较重要的是 mem_fragmentation_ratio = used_memory_rss/used_memory
当 mem_fragmentation_ratio > 1 说明有内存碎片   

当 used_memory_rss/used_memory < 1 说明有内存被交换到磁盘上了，**这个要特别注意**，因为磁盘io很耗时。


#### 2.内存分类
##### 1. 对象内存
存放用户所有数据，每次创建键值对 至少需要创建2个对象 key对象和value对象。所以健也需要消耗内存，**应该避免长健**

##### 2. 缓冲内存
###### 1. 客户端缓冲区
输入缓冲无法控制，输出可以使用client-output-buffer-limit 控制。

1. 普通客户端，一般可以忽略，但是如果客户端很多的话，就不能忽略了。 
下面的参数是不限制
```shell
client-output-buffer-limit normal 0 0 0
```

2. 主从同步
当组从同步的时候，缓冲区最大256M,最小64M,如果持续60秒缓冲区大于64M就关闭客户端。

```shell
client-output-buffer-limit slave 256mb 64mb 60
```

3. 订阅发布的客户端
缓冲区最大8M,最小2M,如果持续60秒缓冲区大于2MM就关闭客户端

```shell
client-output-buffer-limit pubsub 8mb 2mb 60
```
**参考资料**
https://blog.csdn.net/damanchen/article/details/101075757
https://www.cnblogs.com/pk-tiger/p/11866423.html



##### 2. 复制积压缓冲区
复制积压缓冲区 一个redis实例只有一个，用于psync，可以设置大一点例如，防止全量同步。
***repl-backlog-size 100MB**


##### 3. aof缓冲区 
在aof重写期间保存最近的写命令，


#### 3. 子进程内存消耗
在fork的时候，回使用copy-on-write的时候，如果有新的写命令会复制一份父进程修改页的完整拷贝。如果开启了Linux的内存的Transparent Huge Page(THP) 机制，在复制的时候，每次复制4KB内存会变成复制2MB 这会大大增加内存消耗。所以需要关闭THP机制。 
**在共并发的情况下，开启THP 很可能触发swap或者OOM**



#### 4. 管理内存
##### 1. maxmemory设置最大可以使用内存
maxmemory限制的是redis实际使用的内存，也就是used_memory对应的内存。但是由于**内存碎片的存在**，实际消耗的内存比maxmemory大。
所以需要预留一些内存。
##### 2. 可以动态调整 maxmemory 

##### 3. 内存回收-删除过期健  

redis删除过期健使用**惰性删除**加**定时删除**

1. 惰性删除
当客户端读取一个有超时属性的key时候，如果已经过期了，会执行删除并返回空。
2. 定时使出
redis内部有一个定时任务，默认10s执行一次

##### 3. 内存回收-内存溢出策略
1. noeviction
不删数据 ，拒绝所有写入操作

2. allkey-lru 
lru算法删除,直到腾出足够空间

3. allkeys-random 
随机算法删除,直到腾出足够空间

4. volatile-lru 
lru算法删除过期健,直到腾出足够空间，如果没有就退回到noeviction

5. volatile-random 
随机删除过期健,直到腾出足够空间，如果没有就退回到noeviction

6. volatile-ttl
根据ttl 删除最近要过期健,直到腾出足够空间，如果没有就退回到noeviction




###  哨兵Sentinel

#### 1. maser-slave 模式

master slave 模式 下当master出现问题了，需要进行手动主从切换,**无法做到高可用**
步骤为:
1. 对某个从节点执行 slaveof no one
2. 将客户端指向新的主节点
3. 调整其他redis从节点去复制新的主节点，如果原来的主节点恢复，也去复制新的主节点。

#### 2. 哨兵模式 实现高可用

#### 1. 基本
1. 哨兵模式是一个分布式架构，包括sentinel节点和redis节点。
2. 每一个哨兵(Sentinel)都会其他哨兵和redis节点(主节点和从节点)进行监控。
3. 当发现有节点不可达的时候，对其进行下线操作。
4. 当redis主节点不可达的时候，Sentinel会选举一个leader来进行故障转移，并将变化通知应用方(client)


#### 2. 例子
1. redis集群: 1个master和1个slave组成的      
2. sentinel集群: 3个Sentinel实例

##### redis配置
master 在 6479端口
slave  在 6480端口
1. master配置
```shell
port 6479
#daemonize yes
#logfile "logs/6480.log"
dbfilename "dump6480.rdb"
dir "/home/www/programs/redis/master-slave/data"
```

2. slave配置
```shell
port 6480
#daemonize yes
#logfile "logs/6480.log"
dbfilename "dump6480.rdb"
dir "/home/www/programs/redis/master-slave/data"

slaveof 127.0.0.1 6479

```

3. 启动后的redis集群信息
```shell
[www@localhost master-slave]$ redis-cli -p 6479  info replication
# Replication
role:master
connected_slaves:1
slave0:ip=127.0.0.1,port=6480,state=online,offset=15,lag=0
master_repl_offset:15
repl_backlog_active:1
repl_backlog_size:5000000
repl_backlog_first_byte_offset:2
repl_backlog_histlen:14
[www@localhost master-slave]$

```

##### 哨兵配置
```shell
port 26379
logfile "logs/sentinel-26379.log"
dir "/home/www/programs/redis/sentinel/data"

#sentinel监控127.0.0.1 6479 这个主节点，判断主节点失败 至少需要2个sentinel同意 mymaster是主节点的别名
sentinel monitor mymaster 127.0.0.1 6479 2

#每个sentinel节点定期发送ping判断redis数据节点和其他Sentinel节点，如果在下面时间没有回复，就判断为节点不可达
sentinel down-after-milliseconds mymaster 30000

#Sentinel对redis进行故障转移时候，原来的从节点会向新的主节点发起复制，这个参数控制同时发起复制的个数
sentinel parallel-syncs mymaster 1

#故障转移超时时间
sentinel failover-timeout mymaster 180000

```

```shell
port 26380
logfile "logs/sentinel-26380.log"
dir "/home/www/programs/redis/sentinel/data"

#sentinel监控127.0.0.1 6479 这个主节点，判断主节点失败 至少需要2个sentinel同意 mymaster是主节点的别名
sentinel monitor mymaster 127.0.0.1 6479 2

#每个sentinel节点定期发送ping判断redis数据节点和其他Sentinel节点，如果在下面时间没有回复，就判断为节点不可达
sentinel down-after-milliseconds mymaster 30000

#Sentinel对redis进行故障转移时候，原来的从节点会向新的主节点发起复制，这个参数控制同时发起复制的个数
sentinel parallel-syncs mymaster 1

#故障转移超时时间
sentinel failover-timeout mymaster 180000
```

```shell
port 26381
logfile "logs/sentinel-26381.log"
dir "/home/www/programs/redis/sentinel/data"

#sentinel监控127.0.0.1 6479 这个主节点，判断主节点失败 至少需要2个sentinel同意 mymaster是主节点的别名
sentinel monitor mymaster 127.0.0.1 6479 2

#每个sentinel节点定期发送ping判断redis数据节点和其他Sentinel节点，如果在下面时间没有回复，就判断为节点不可达
sentinel down-after-milliseconds mymaster 30000

#Sentinel对redis进行故障转移时候，原来的从节点会向新的主节点发起复制，这个参数控制同时发起复制的个数
sentinel parallel-syncs mymaster 1

#故障转移超时时间
sentinel failover-timeout mymaster 180000
```

##### 启动哨兵集群
```shell
[www@localhost sentinel]$ cat start_all.sh 
nohup ../redis-3.0.7/src/redis-server redis-sentinel-26379.conf  --sentinel >> logs/26379.log  2>&1 &
nohup ../redis-3.0.7/src/redis-server redis-sentinel-26380.conf  --sentinel >> logs/26380.log  2>&1 &
nohup ../redis-3.0.7/src/redis-server redis-sentinel-26381.conf --sentinel >> logs/26381.log  2>&1 &
[www@localhost sentinel]$ bash start_all.sh 
[www@localhost sentinel]$ ps -aux | grep redis
www       55639  0.0  0.2  20164  5160 pts/2    S+   15:39   0:00 redis-cli -p 6480
www       56396  0.1  0.4 149036  9996 pts/0    Sl   16:01   0:00 ../redis-3.0.7/src/redis-server *:6479
www       56397  0.1  0.3 140844  7712 pts/0    Sl   16:01   0:00 ../redis-3.0.7/src/redis-server *:6480
www       56536  1.0  0.3 140840  7700 pts/1    Sl   16:03   0:02 ../redis-3.0.7/src/redis-server *:26380 [sentinel]
www       56537  0.9  0.3 140840  7752 pts/1    Sl   16:03   0:01 ../redis-3.0.7/src/redis-server *:26381 [sentinel]
www       56689  0.8  0.1 140840  4300 pts/1    Sl   16:06   0:00 ../redis-3.0.7/src/redis-server *:26379 [sentinel]


```

##### 验证哨兵
```shell
[www@localhost sentinel]$  redis-cli  -p 26379 info Sentinel
# Sentinel
sentinel_masters:1
sentinel_tilt:0
sentinel_running_scripts:0
sentinel_scripts_queue_length:0
master0:name=mymaster,status=ok,address=127.0.0.1:6479,slaves=2,sentinels=3
[www@localhost sentinel]$ 

```

获取名字为mymaster集群的master信息
```shell
[www@localhost sentinel]$ redis-cli -p 26379
26379> sentinel get-master-addr-by-name mymaster
1) "127.0.0.1"
2) "6479"

```
#### sentinel 几个重要参数

##### 1. sentinel monitor<mastername> <ip> <port> <quorum>
例如 sentinel monitor mymaster 127.0.0.1 6479 2
1. sentinel监控127.0.0.1 6479 这个主节点
2. 判断主节点不可达 至少需要2个sentinel同意 
3. mymaster是主节点的别名
4. 至少需要 MAX(quorum，sentinel节点个数/2+1) 个节点统一才能选出一个sentinel领导者

##### 2. sentinel down after milliseconds <mastername> <times>
例如 sentinel down-after-milliseconds mymaster 30000
1. 每个sentinel节点定期发送ping判断redis数据节点和其他Sentinel节点，如果在下面时间没有回复，就判断为节点不可达

##### 3.sentinel parallel syncs <mastername> <nums>
sentinel parallel-syncs mymaster 1
1. Sentinel对redis进行故障转移时候，原来的从节点会向新的主节点发起复制，这个参数控制同时发起复制的个数
2. 设置得太大可能会将master的io打满


##### 4. sentinel failover-timeout  <mastername> <times>
例如  sentinel failover-timeout mymaster 180000

下面是故障转移的过程：  
a）选出合适从节点。  
b）晋升选出的从节点为主节点(执行 slaveof no one)
c）命令其余从节点复制新的主节点。  
d）等待原主节点恢复后命令它去复制新的主节点。  


1. 如果对主节点故障转移失败，俺么下次再对该主节点做故障转移的时间是2*failover-time. **相当于一个退避**
2. 如果在 b阶段执行slaveof no one失败(从节点失败) 当此过程抄错failover-timeout时候，就认为故障转移失败。
3. b阶段成功，sentinel会执行 info 确认从节点晋升为master成功，如果此过程执行时间超过 failover-time,就认为故障转移失败。
4. c阶段执行时间超过failover-time(不包括复制时间)，就认为故障转移失败。 



##### 5. sentinel  notification-script <master-name> <script-path>
这个参数是在 故障转移期间，一些告警界别的Sentinel事件，例如主观下线，客观下线，会复发这个脚本。
script 如下所示
```shell
#! /bin/sh
#获取参数
msg=$*
echo msg
exit 0
[www@localhost sentinel]$ 

```

##### 5. sentinel client-reconfig-script <master-name> <script-path>
在故障转移结束后，发送转移结果的相关参数


 ![部署](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/sentinel-set.png) 



#### sentinel可以监控多个redis集群
```shell
port 26379
logfile "logs/sentinel-26379.log"
dir "/home/www/programs/redis/sentinel/data"

#mymaster 第一个集群
sentinel monitor mymaster 127.0.0.1 6479 2
sentinel down-after-milliseconds mymaster 30000
sentinel parallel-syncs mymaster 1
sentinel failover-timeout mymaster 180000

#mymaster1 第二个集群
sentinel monitor mymaster1 127.0.0.1 6979 2
sentinel down-after-milliseconds mymaster1 20000
sentinel parallel-syncs mymaster1 1
sentinel failover-timeout mymaster1 180000

```

#### sentinel客户端原理
1. 遍历sentinel节点获取一个可用的
2. 通过sentinel get-master-addr-by-name master-name这个api获取对应主节点的相关信息。
3. 验证主节点(role 或者 info replication)是真正主节点(为了防止在故障转移期间主节点的变化)

#### sentinel 客户端操作

##### 1. sentinel masters
展示所有被监控的master状态与相关统计信息

##### 2. sentinel master <master name>
展示名字为 <master name>的主节点以及相关统计信息

##### 3. sentinel slaves <master name>
展示主节点名字为 <master name>的从节点以及相关统计信息

##### 4. sentinel sentinels <master name>
展示指定<master name>的sentinels节点集合

##### 5. sentinel get-master-addr-by-name <master name>
展示指定<master name>的主节点的ip和端口

##### 6. sentinel reset <pattern>
对符合pattern的主节点的配置进行重置，包括清除主节点相关状态，从新发现从节点和sentinel节点。

##### 7 sentinel failover <master name>
最指定的<master name> 的主节点进行强制故障转移(不用和其他sentinel协商)，当故障转移后其他sentinel节点找故障转移结果进行更新自己信息。

##### 8 sentinel ckquorum <master name>
检查可达的sentinel是否达到<quorum>的个数。如果返回ok表示可以进行故障转移。

##### 9 sentinel flushconfig
将sentinel的配置刷新到磁盘上

##### 10 sentinel remove <master name>
取消对<master name> 的主节点的监控，仅仅对当前sentinel有效。

##### 11 sentinel monitor <master name> <ip> <port> <quorum>
和配置中一样 

##### 12 sentinel is-master-down-by-addr
sentinel节点用来交换对主节点是否下线的判断，还可以用来选举领导者。



#### java客户端。
```java
package andy.com.internet.redis;

import redis.clients.jedis.*;

import java.util.*;

public class JedisSentinelManager {
    private int maxTotal = 200;
    private int poolMaxWaitMs = 10;
    private int timeOutMs = 100;
    private String ip;
    private int port;
    private String password;

    private JedisSentinelPool jedisPool = null;


    public void init() {

        JedisPoolConfig config = new JedisPoolConfig();
        config.setMaxTotal(maxTotal);
        config.setMaxWaitMillis(poolMaxWaitMs);
        config.setMaxIdle(maxTotal);
        config.setMinIdle(4);
        config.setBlockWhenExhausted(true);
        config.setTestWhileIdle(true);
        if (password == null || password.trim().equals("")) {
            password = null;
        }

        Set<String> sentinels = new HashSet<>();
        sentinels.add("192.168.231.149:26379");
        sentinels.add("192.168.231.149:26380");
        sentinels.add("192.168.231.149:26381");

        jedisPool = new JedisSentinelPool("mymaster",sentinels, config);
    }

    public void destroy() {
        jedisPool.close();
    }

    public Jedis getJedis() {
        return jedisPool.getResource();
    }


    public String get(String key) {

        try (Jedis jedis = getJedis()) {
            return jedis.get(key);
        } catch (Exception e) {
            throw e;
        }
    }

    public void set(String key, String value, int expire) {
        try (Jedis jedis = getJedis()) {
            jedis.setex(key, expire, value);
        } catch (Exception e) {
            throw e;
        }
    }

    public Long del(String key) {

        try (Jedis jedis = getJedis()) {
            return jedis.del(key);
        } catch (Exception e) {
            throw e;
        }
    }



}

```

#### sentinel原理
##### 三个定时任务
###### 1. 每隔10每个sentinel会向主节点和从节点发送info命令获取拓扑结构。
这个定时任务可以完成    
1. 像master发送info可以获取从节点信息。(所以sentinel不用显示配置从节点)
2. 当有新的从节点加入时候，会自动感知出来。
3. 当节点不可达或者故障转移后，可以通过info命令试试更新拓扑信息。


###### 2. 每个2秒每个sentinel节点像数据节点的__sentinel__:hello 频道上发送该sentinel对主节点的判断，同事也会订阅该频道。
这个定时任务可以完成    

1. 发现新的sentinel节点，并与其建立连接(通过订阅主节点的__sentinel__:hello了解其他主节点的信息。)
2. sentinel节点之间交换主节点的状态。

###### 3. 每隔1秒，每隔sentinel会像主节点，从节点，其余sentinel发送一条ping命令作为心跳检测。
sentinel会对其他所有的sentinel， 主节点，从节点进行监控。


##### 下线
###### 1. 主观下线
 sentinel没1秒对主节点，从节点，和其他sentinel发送ping，如果这些节点操作了down-after-milliseconds都没有回复。就认为节点失败，这叫做**主观下线**
###### 2. 客观下线
 如果主观下线的是主节点，该sentinel会向其他的sentinel发送**sentinel is-master-down-by-addr**向其他主节点询问对主节点是否下线的判断。如果超过了<quorum>个都做出下线决定，就叫做**客观下线**

###### 3. 选举
sentinel使用raft算法实现领导选举leader，由leader来进行下线处理。


