---
layout: post
title:  kafka基本概念与设计
date:   2019-6-21 14:32:00
categories: 消息队列 kafka
---

用kafka这么久了，还是系统理一下

![kafka](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/2019-06-21-kafka-1.png)

![下载](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/files/kafka-基本概念与设计.mindnode)

# 1. kafka基础
## 1. kafak基本概念
### 1. 分区模型
1. 集群有多个消息代理服务器(broker server)组成。
2. 每一条消息都有一个topic(主题)
3. 在broker上每一个祖逖有多个分区(partition),partition“均匀”分布在各个broker服务器上(可以看做是做了负载均衡)。
4. 每一个发送到分区里的消息都有一个单调递增的需要叫做偏移量offset。不同分区的offset都是独立的互不影响。
5. 每一个分区只能分给一个消费者，一个消费者可以有多个分区。 所以并行消费的基本单元是分区。

### 2. 消费模型
1. mq有 push 和 pull 两种消费模型，kafka使用pull模型
2. 消费者自己记录消费状态，好处是消费者可以跳过一些消息，或者回溯重消费一些消息。
3. kafka 会存储消息，不管消息有没有被消费。可以设置策略来清理过期的消息。比如只保留7天的消息。


### 3.分布式 模型
1. 每个topic的分区日志都会分布式存在kafka集群里。
2. 每个分区都可以设置多个副本存储在其他broker上。有一个分区为主副本(leader)，其他为备份副本(follower),**主副本复制所有客户端的读写操作**，被副本只从leader同步数据
3. 当分区的主副本所在的broker出现故障，备份副本会选一个新的主副本。
4. 每个分区都在不同的broker上，kafak集群对于客户端整体上是负载均衡的。
5. **生产者**发送消息到某个topic时候，如果消息有**键**，根据键值的hash将消息发送到某个partition；如果没有指定键，使用round-robin算法指定partition.
6. **消费者**必须指定一个 **消费组名称**，每一条生产者的消息都会发送到**消费组的一消费者**。
7. **消费者**通过**消费组**可以实现**队列模式**和**发布-订阅模式**，如果每个消费者的**消费组**都一样，那么一条消息只有一个消费者能收到(队列模式)。如果每个消费者的消费组不一样，这样每个消费者都会收到消息(发布-订阅模式)。
8. kafka会将所有分区平均分配给某个消费组里的所有消费者实例，当一个新消费者加入，或者离开的时候，kafka消费组管理协议会出发**再平衡(rebalance)**操作。
9. kafka值保证分区内的消息完全有序，不保同一个主题多个分区消费有序。可以加上key来让同一个key的消息发送到同一个分区。

## 2. kafka实现
### 1. 文件系统持久化和数据传输
1. 顺序写
2. 预读
3. 合并写

4. 读取的时候使用0拷贝

### 2. 生成者与消费者
1. 所有broker保存一份元数据(记录每个主题分区的主副本所在节点),生产者直接将消息发送到主副本所在的broker节点，没有中间路由
2. 生产者搜集到足够多的数据后，采用批量发送消息，减少网络请求。 例如 100ms内大于64字节就立即发送，大于100ms或者大于64页立即发送。**这样增加了一点延迟换来了更高的吞吐量**
3. push模式的话，broker需要记录每条消息的消费装填。消费者使用pull模式，这样broker是无状态的，不同消费者可以按照自己的最大处理能力来拉取数据。
4. kafak只需要记录每个partiion的一个消费进度。

### 3. 容错
1. kafak的分区数据有多个副本，存放在不同的broker上。每一个分区有一个主副本和若干个备份副本，备份副本和主副本保持同步。
2. 当某个节点出现故障，对故障节点的请求会转移到其他节点上。
3. 如果1个节点和zookeeper保持回话 并且是某个分区备份副本的话，ta同步主副本没有落后太多，就叫做“in-sync”。每一个分区的主副本会跟踪in-sync的备份副本节点(ISR = In Sync Replica)。如果一个备份副本挂掉或者落后太多，主副本就会将其从ISR中移除。
4. 一个消息只有被ISR中所有副本都写入本地日志，才能认为消费被**成功提交**。
5. 一个消息只有**成功提交**后才对消费者可见。

# 1. kafa
三台机器
|name|端口|
|---|---|
|server-1| 9093|
|server-2| 9094|
|server-3| 9095|


```java
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions  5 --topic topic1 
Created topic "topic1".
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic1Topic:topic1	PartitionCount:5	ReplicationFactor:3	Configs:
	Topic: topic1	Partition: 0	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2
	Topic: topic1	Partition: 1	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: topic1	Partition: 2	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
	Topic: topic1	Partition: 3	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3
	Topic: topic1	Partition: 4	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1

```



**第一条命令**创建了一个名字为topic1的主题，这个主题有5个partition，副本数为3。
**第二条命令**查看了当前topic的状态
比如
> topic: topic1	Partition: 0	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2

partition 0 分布在 1 3 2 这三台机器上，其中leader 为1这台机器,并且leader负责这个partition的读写。

partition 0和3 的 leader在server1 
partition 1和4 的 leader在server12
partition 2 的 leader在server3

isr = in sync replica, 表示某个partition褒词同步的server，partition 0 的leader是1，并且副本在2，3这两台机器上保持了同步。

读写partition0的consumer和读写取partition1的consumer会 在不同的机器上，可以看到leader在整个集群中做了**负载均衡**。



**杀掉 server-1**后再看看集群的情况
```java
[www@localhost local-cluster]$ jps -lm
14161 org.apache.zookeeper.server.quorum.QuorumPeerMain configs/zookeeper.properties
14467 kafka.Kafka configs/server-1.properties
14468 kafka.Kafka configs/server-2.properties
14469 kafka.Kafka configs/server-3.properties
17710 sun.tools.jps.Jps -lm
[www@localhost local-cluster]$ kill -9 14467
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic1
Topic:topic1	PartitionCount:5	ReplicationFactor:3	Configs:
	Topic: topic1	Partition: 0	Leader: 3	Replicas: 1,3,2	Isr: 3,2
	Topic: topic1	Partition: 1	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: topic1	Partition: 2	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
	Topic: topic1	Partition: 3	Leader: 2	Replicas: 1,2,3	Isr: 2,3
	Topic: topic1	Partition: 4	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1

```


我们可以看到  leader有很大的变化，集群做了**rebalance(重平衡)**, 将partition的leader做了调整。



**将server 1重新启动起来**
```java
[www@localhost local-cluster]$ nohup ../kafka_2.11-2.1.1/bin/kafka-server-start.sh configs/server-1.properties >> logs/kafka1.log &
[2] 18764
[www@localhost local-cluster]$ nohup: 忽略输入重定向错误到标准输出端

[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic1
Topic:topic1	PartitionCount:5	ReplicationFactor:3	Configs:
	Topic: topic1	Partition: 0	Leader: 3	Replicas: 1,3,2	Isr: 3,2,1
	Topic: topic1	Partition: 1	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: topic1	Partition: 2	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
	Topic: topic1	Partition: 3	Leader: 2	Replicas: 1,2,3	Isr: 2,3,1
	Topic: topic1	Partition: 4	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1

```
重新加入server1后，isr都加上了1.
这个时候我们没有往kafka写数据。leader还是保持原来的样子，没有执行rebalance操作。我们可以手动执行rebalance操作
```java
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181
Created preferred replica election path with topic1-2,topic1-4,topic1-0,topic1-3,topic1-1
Successfully started preferred replica election for partitions Set(topic1-2, topic1-4, topic1-0, topic1-3, topic1-1)
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic1
Topic:topic1	PartitionCount:5	ReplicationFactor:3	Configs:
	Topic: topic1	Partition: 0	Leader: 1	Replicas: 1,3,2	Isr: 3,2,1
	Topic: topic1	Partition: 1	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: topic1	Partition: 2	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
	Topic: topic1	Partition: 3	Leader: 1	Replicas: 1,2,3	Isr: 2,3,1
	Topic: topic1	Partition: 4	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1

```

文档
https://www.cnblogs.com/senlinyang/p/8124322.html


