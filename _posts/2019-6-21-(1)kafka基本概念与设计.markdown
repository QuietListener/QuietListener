---
layout: post
title:  kafka基本概念与设计
date:   2019-6-21 14:32:00
categories: 消息队列 kafka
---

用kafka这么久了，还是系统理一下

![kafka](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/images/2019-06-21-kafka-1.png)

![下载](https://raw.githubusercontent.com/QuietListener/quietlistener.github.io/master/files/kafka-基本概念与设计.mindnode)

# 1. kafka基础
## 1. kafak基本概念
### 1. 分区模型
1. 集群有多个消息代理服务器(broker server)组成。
2. 每一条消息都有一个topic(主题)
3. 在broker上每一个祖逖有多个分区(partition),partition“均匀”分布在各个broker服务器上(可以看做是做了负载均衡)。
4. 每一个发送到分区里的消息都有一个单调递增的需要叫做偏移量offset。不同分区的offset都是独立的互不影响。
5. 每一个分区只能分给一个消费者，一个消费者可以有多个分区。 所以并行消费的基本单元是分区。

### 2. 消费模型
1. mq有 push 和 pull 两种消费模型，kafka使用pull模型
2. 消费者自己记录消费状态，好处是消费者可以跳过一些消息，或者回溯重消费一些消息。
3. kafka 会存储消息，不管消息有没有被消费。可以设置策略来清理过期的消息。比如只保留7天的消息。


### 3.分布式 模型
1. 每个topic的分区日志都会分布式存在kafka集群里。
2. 每个分区都可以设置多个副本存储在其他broker上。有一个分区为主副本(leader)，其他为备份副本(follower),**主副本复制所有客户端的读写操作**，被副本只从leader同步数据
3. 当分区的主副本所在的broker出现故障，备份副本会选一个新的主副本。
4. 每个分区都在不同的broker上，kafak集群对于客户端整体上是负载均衡的。
5. **生产者**发送消息到某个topic时候，如果消息有**键**，根据键值的hash将消息发送到某个partition；如果没有指定键，使用round-robin算法指定partition.
6. **消费者**必须指定一个 **消费组名称**，每一条生产者的消息都会发送到**消费组的一消费者**。
7. **消费者**通过**消费组**可以实现**队列模式(1对1)**和**发布-订阅模式**，如果每个消费者的**消费组**都一样，那么一条消息只有一个消费者能收到(队列模式)。如果每个消费者的消费组不一样，这样每个消费者都会收到消息(发布-订阅模式)。
8. kafka会将所有分区平均分配给某个消费组里的所有消费者实例，当一个新消费者加入，或者离开的时候，kafka消费组管理协议会出发**再平衡(rebalance)**操作。
9. kafka值保证分区内的消息完全有序，不保同一个主题多个分区消费有序。可以加上key来让同一个key的消息发送到同一个分区。

## 2. kafka实现
### 1. 文件系统持久化和数据传输
1. 顺序写
2. 预读
3. 合并写

4. 读取的时候使用0拷贝

### 2. 生成者与消费者
1. 所有broker保存一份元数据(记录每个主题分区的主副本所在节点),生产者直接将消息发送到主副本所在的broker节点，没有中间路由
2. 生产者搜集到足够多的数据后，采用批量发送消息，减少网络请求。 例如 100ms内大于64字节就立即发送，大于100ms或者大于64页立即发送。**这样增加了一点延迟换来了更高的吞吐量**
3. push模式的话，broker需要记录每条消息的消费装填。消费者使用pull模式，这样broker是无状态的，不同消费者可以按照自己的最大处理能力来拉取数据。
4. kafak只需要记录每个partiion的一个消费进度。

### 3. 容错
1. kafak的分区数据有多个副本，存放在不同的broker上。每一个分区有一个主副本和若干个备份副本，备份副本和主副本保持同步。
2. 当某个节点出现故障，对故障节点的请求会转移到其他节点上。
3. 如果1个节点和zookeeper保持回话 并且是某个分区备份副本的话，ta同步主副本没有落后太多，就叫做“in-sync”。每一个分区的主副本会跟踪in-sync的备份副本节点(ISR = In Sync Replica)。如果一个备份副本挂掉或者落后太多，主副本就会将其从ISR中移除。
4. 一个消息只有被ISR中所有副本都写入本地日志，才能认为消费被**成功提交**。
5. 一个消息只有**成功提交**后才对消费者可见。

# 1. kafa
三台机器
|name|端口|
|---|---|
|server-1| 9093|
|server-2| 9094|
|server-3| 9095|


```java
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 3 --partitions  5 --topic topic1 
Created topic "topic1".
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic1Topic:topic1	PartitionCount:5	ReplicationFactor:3	Configs:
	Topic: topic1	Partition: 0	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2
	Topic: topic1	Partition: 1	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: topic1	Partition: 2	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
	Topic: topic1	Partition: 3	Leader: 1	Replicas: 1,2,3	Isr: 1,2,3
	Topic: topic1	Partition: 4	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1

```



**第一条命令**创建了一个名字为topic1的主题，这个主题有5个partition，副本数为3。
**第二条命令**查看了当前topic的状态
比如
> topic: topic1	Partition: 0	Leader: 1	Replicas: 1,3,2	Isr: 1,3,2

partition 0 分布在 1 3 2 这三台机器上，其中leader 为1这台机器,并且leader负责这个partition的读写。

partition 0和3 的 leader在server1 
partition 1和4 的 leader在server12
partition 2 的 leader在server3

isr = in sync replica, 表示某个partition褒词同步的server，partition 0 的leader是1，并且副本在2，3这两台机器上保持了同步。

读写partition0的consumer和读写取partition1的consumer会 在不同的机器上，可以看到leader在整个集群中做了**负载均衡**。



**杀掉 server-1**后再看看集群的情况
```java
[www@localhost local-cluster]$ jps -lm
14161 org.apache.zookeeper.server.quorum.QuorumPeerMain configs/zookeeper.properties
14467 kafka.Kafka configs/server-1.properties
14468 kafka.Kafka configs/server-2.properties
14469 kafka.Kafka configs/server-3.properties
17710 sun.tools.jps.Jps -lm
[www@localhost local-cluster]$ kill -9 14467
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic1
Topic:topic1	PartitionCount:5	ReplicationFactor:3	Configs:
	Topic: topic1	Partition: 0	Leader: 3	Replicas: 1,3,2	Isr: 3,2
	Topic: topic1	Partition: 1	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: topic1	Partition: 2	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
	Topic: topic1	Partition: 3	Leader: 2	Replicas: 1,2,3	Isr: 2,3
	Topic: topic1	Partition: 4	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1

```


我们可以看到  leader有很大的变化，集群做了**rebalance(重平衡)**, 将partition的leader做了调整。



**将server 1重新启动起来**
```java
[www@localhost local-cluster]$ nohup ../kafka_2.11-2.1.1/bin/kafka-server-start.sh configs/server-1.properties >> logs/kafka1.log &
[2] 18764
[www@localhost local-cluster]$ nohup: 忽略输入重定向错误到标准输出端

[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic1
Topic:topic1	PartitionCount:5	ReplicationFactor:3	Configs:
	Topic: topic1	Partition: 0	Leader: 3	Replicas: 1,3,2	Isr: 3,2,1
	Topic: topic1	Partition: 1	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: topic1	Partition: 2	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
	Topic: topic1	Partition: 3	Leader: 2	Replicas: 1,2,3	Isr: 2,3,1
	Topic: topic1	Partition: 4	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1

```
重新加入server1后，isr都加上了1.
这个时候我们没有往kafka写数据。leader还是保持原来的样子，没有执行rebalance操作。我们可以手动执行rebalance操作
```java
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-preferred-replica-election.sh --zookeeper localhost:2181
Created preferred replica election path with topic1-2,topic1-4,topic1-0,topic1-3,topic1-1
Successfully started preferred replica election for partitions Set(topic1-2, topic1-4, topic1-0, topic1-3, topic1-1)
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic topic1
Topic:topic1	PartitionCount:5	ReplicationFactor:3	Configs:
	Topic: topic1	Partition: 0	Leader: 1	Replicas: 1,3,2	Isr: 3,2,1
	Topic: topic1	Partition: 1	Leader: 2	Replicas: 2,1,3	Isr: 2,1,3
	Topic: topic1	Partition: 2	Leader: 3	Replicas: 3,2,1	Isr: 3,2,1
	Topic: topic1	Partition: 3	Leader: 1	Replicas: 1,2,3	Isr: 2,3,1
	Topic: topic1	Partition: 4	Leader: 2	Replicas: 2,3,1	Isr: 2,3,1

```

文档
https://www.cnblogs.com/senlinyang/p/8124322.html


消费者
```shell
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-console-consumer.sh --bootstrap-server  localhost:9093 --topic topic1
gekki
nihso
aaa
asdfasf

```

生产者
```shell
@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/kafka-console-producer.sh --broker-list  localhost:9093 --topic topic1
>gekki
>nihso
>asdfasf
>

```

消费信息都保存在zookeeper上
```shell
[www@localhost local-cluster]$ ../kafka_2.11-2.1.1/bin/zookeeper-shell.sh  localhost:2181
Connecting to localhost:2181
Welcome to ZooKeeper!
JLine support is disabled

WATCHER::

WatchedEvent state:SyncConnected type:None path:null
ls /
[cluster, controller_epoch, controller, brokers, zookeeper, admin, isr_change_notification, consumers, log_dir_event_notification, latest_producer_id_block, config]
ls /brokers
[ids, topics, seqid]
```


# 2. 生产者
## 发送特点
1. 不直接发给服务器
2. 在客户端将消息放入队列中，由衣蛾消息发送线程拉取队列中的消息，批量发送给服务器
## 发送同步/异步消息
### 1. 异步发送 
1. 提供一个回调，当小费成功存储到kafka集群后，服务端会调用回调方法。
2. 发送一调消息后不需要等待服务器处理完没有，直接发送下一条
### 2. 同步发送
1.  发送消息后，必须等待服务器返回结果
发送时候返回一个Future，调用future.get() 会一直阻塞

## 发送过程
### 1. 选择分区（负载均衡）
1. 如果有key的按照key来选择分区，相同的key放在相同的分区
2. 如果没有key 采用round-robin方式发送到各个分区
3. **各个分区可以并行发送**到服务器集群~

### 2. 消息序列化

### 3. 将消息缓存到记录收集器中，等待一定时机发送线程Sender 发送给 NetworkClient，由NetworkClient再批量发送给kafka集群。

### 4. NetworkClient将数据发送给kafka集群

# 3. 消费者
## 




# 配置 
## 服务器端
1. log.retention.{hours|minutes|ms} 消息留存时间
2. •log.retention.bytes 消息留存大小
3. unclean.leader.election.enable
该参数默认值是false，即表明如果发生这种情况，Kafka不允许从剩下存活的非ISR副本中选择一个当leader。
4. message.max.bytes 消息最大字节数
5. min.insync.replicas
该参数其实是与producer端的acks参数配合使用的。关于acks含义的介绍，我们留到第4章中详细展开。这里只需要了解acks=1表示producer端寻求最高等级的持久化保证，而min.insync.replicas也只有在acks=1时才有意义。它指定了broker端必须成功响应clients消息发送的最少副本数。假如broker端无法满足该条件，则clients的消息发送并不会被视为成功。它与acks配合使用可以令Kafka集群达成最高等级的消息持久化。在实际使用中如果用户非常在意被发送的消息是否真的成功写入了所有副本，那么推荐将参数设置为副本数1。举一个例子，假设某个topic的每个分区的副本数是3，那么推荐设置该参数为2，这样我们就能够容忍一台broker宕机而不影响服务；若设置参数为3，那么只要任何一台broker宕机，整个Kafka集群将无法继续提供服务。因此用户需要在高可用和数据一致性之间取得平衡。


## Producer参数
### 1. bootstrap.servers 
服务器地址
### 2. key.serializer value.serializer
可以和value进行序列化
### 3. acks(重要)
1. acks=0：设置成0表示producer完全不理睬leaderbroker端的处理结果。此时，producer发送消息后立即开启下一条消息的发送，根本不等待leaderbroker端返回结果。由于不接收发送结果，因此在这种情况下producer.send的回调也就完全失去了作用，即用户无法通过回调机制感知任何发送过程中的失败，所以acks=0时producer并不保证消息会被成功发送。但凡事有利就有弊，由于不需要等待响应结果，通常这种设置下producer的吞吐量是最高的
2. acks=all或者1：表示当发送消息时，leaderbroker不仅会将消息写入本地日志，同时还会等待ISR中所有其他副本都成功写入它们各自的本地日志后，才发送响应结果给producer。显然当设置acks=all时，只要ISR中至少有一个副本是处于“存活”状态的，那么这条消息就肯定不会丢失，因而可以达到最高的消息持久性，但通常这种设置下producer的吞吐量也是最低的。
3. acks=1：是0和all折中的方案，也是默认的参数值。producer发送消息后leaderbroker仅将该消息写入本地日志，然后便发送响应结果给producer，而无须等待ISR中其他副本写入该消息。那么此时只要该leaderbroker一直存活，Kafka就能够保证这条消息不丢失。这实际上是一种折中方案，既可以达到适当的消息持久性，同时也保证了producer端的吞吐量。

### 4. retries
### 5. max.in.flight.requests.per.connection=1
设置该参数为1主要是为了防止topic同分区下的消息乱序问题。这个参数的实际效果其实限制了producer在单个broker连接上能够发送的未响应请求的数量。因此，如果设置成1，则producer在某个broker发送响应之前将无法再给该broker发送PRODUCE请求。
### 6. compression.type
### 7. batch.size(重要)
batch.size是producer最重要的参数之一！它对于调优producer吞吐量和延时性能指标都有着非常重要的作用。前面提到过，producer会将发往同一分区的多条消息封装进一个batch中。当batch满了的时候，producer会发送batch中的所有消息。不过，producer并不总是等待batch满了才发送消息，很有可能当batch还有很多空闲空间时producer就发送该batch。显然，batch的大小就显得非常重要。通常来说，一个小的batch中包含的消息数很少，因而一次发送请求能够写入的消息数也很少，所以producer的吞吐量会很低；但若一个batch非常之巨大，那么会给内存使用带来极大的压力，因为不管是否能够填满，producer都会为该batch分配固定大小的内存。因此batch.size参数的设置其实是一种时间与空间权衡的体现。batch.size参数默认值是16384，即16KB。这其实是一个非常保守的数字。在实际使用过程中合理地增加该参数值，通常都会发现producer的吞吐量得到了相应的增加。

### 8. linger.ms(重要)
上面说到batch.size时，我们提到了消息没有填满batch也可以被发送的情况。这是为什么呢？难道不是等batch满了再发送比较好吗？实际上这也是一种权衡，即吞吐量与延时之间的权衡。linger.ms参数就是控制消息发送延时行为的。该参数默认值是0，表示消息需要被立即发送，无须关心batch是否已被填满，大多数情况下这是合理的，毕竟我们总是希望消息被尽可能快地发送。不过这样做会拉低producer吞吐量，毕竟producer发送的每次请求中包含的消息数越多，producer就越能将发送请求的开销摊薄到更多的消息上，从而提升吞吐量。

### 9.max.request.size
官网中给出的解释是，该参数用于控制producer发送请求的大小。实际上该参数控制的是producer端能够发送的最大消息大小。由于请求有一些头部数据结构，因此包含一条消息的请求的大小要比消息本身大。不过姑且把它当作请求的最大尺寸是安全的。如果producer要发送尺寸很大的消息，那么这个参数就是要被设置的。默认的1048576字节太小了，通常无法满足企业级消息的大小要求。


## Consumer参数
### 1. session.timeout.ms
consumer消息处理逻辑的最大时间——倘若consumer两次poll之间的间隔超过了该参数所设置的阈值，那么coordinator就会认为这个consumer已经追不上组内其他成员的消费进度了，因此会将该consumer实例“踢出”组，该consumer负责的分区也会被分配给其他consumer。在最好的情况下，这会导致不必要的rebalance，因为consumer需要重新加入group。更糟的是，对于那些在被踢出group后处理的消息，consumer都无法提交位移——这就意味着这些消息在rebalance之后会被重新消费一遍。

### 2. max.poll.interval.ms
这个参数就是用于设置消息处理逻辑的最大时间的。假设用户的业务场景中消息处理逻辑是把消息“落地”到远程数据库中，且这个过程平均处理时间是2分钟，那么用户仅需要将max.poll.interval.ms设置为稍稍大于2分钟的值即可，而不必为session.timeout.ms也设置这么大的值。

### 3. auto.offset.reset
指定了无位移信息或位移越界（即consumer要消费的消息的位移不在当前消息日志的合理区间范围）时Kafka的应对策略。特别要注意这里的无位移信息或位移越界，只有满足这两个条件中的任何一个时该参数才有效果。

目前该参数有如下3个可能的取值：  
1. earliest：指定从最早的位移开始消费。注意这里最早的位移不一定就是0。
2. latest：指定从最新处位移开始消费。
3. none：指定如果未发现位移信息或位移越界，则抛出异常。笔者在实际使用过程中几乎从未见过将该参数设置为none的用法，因此该值在真实业务场景中使用甚少。

###  4. enable.auto.commit
该参数指定consumer是否自动提交位移。若设置为true，则consumer在后台自动提交位移；否则，用户需要手动提交位移。对于有较强“精确处理一次”语义需求的用户来说，最好将该参数设置为false，由用户自行处理位移提交问题。

### 5. max.poll.records
单词拉最大数量

### 6. heartbeat.interval.ms
这里的关键在于要搞清楚consumergroup的其他成员如何得知要开启新一轮rebalance——当coordinator决定开启新一轮rebalance时，它会将这个决定以REBALANCE_IN_PROGRESS异常的形式“塞进”consumer心跳请求的response中，这样其他成员拿到response后才能知道它需要重新加入group。显然这个过程越快越好，而heartbeat.interval.ms就是用来做这件事情的。
